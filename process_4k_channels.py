import os
import sys
import time
import requests
import re
import statistics
from typing import Dict, List, Tuple, Optional
import concurrent.futures

# å®šä¹‰è¦å¤„ç†çš„æ–‡ä»¶è·¯å¾„
FILE_PATH = '4K_uhd_channels.txt'

# è¯·æ±‚å¤´
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
}

# æµ‹é€Ÿé…ç½®
SPEED_TEST_CONFIG = {
    'timeout': 10,           # è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    'test_duration': 5,      # æµ‹é€ŸæŒç»­æ—¶é—´ï¼ˆç§’ï¼‰
    'max_workers': 10,       # å¹¶å‘å·¥ä½œçº¿ç¨‹æ•°
    'min_download_size': 1024 * 1024,  # æœ€å°ä¸‹è½½å¤§å°ï¼ˆå­—èŠ‚ï¼‰
    'speed_test_enabled': True  # æ˜¯å¦å¯ç”¨æµ‹é€ŸåŠŸèƒ½
}

# éªŒè¯URLæ˜¯å¦æœ‰æ•ˆå¹¶æµ‹é€Ÿ
def test_url_speed(url, timeout=10):
    """æµ‹è¯•URLçš„é€Ÿåº¦ï¼Œè¿”å›æœ‰æ•ˆæ€§å’Œé€Ÿåº¦ä¿¡æ¯"""
    try:
        start_time = time.time()
        downloaded_size = 0
        chunks = []
        
        # å¯¹GitHubåŸå§‹æ–‡ä»¶URLæ·»åŠ ghfast.topå‰ç¼€
        if url.startswith('https://raw.githubusercontent.com/'):
            test_url = f"https://ghfast.top/{url}"
        else:
            test_url = url
        
        # ä½¿ç”¨streamæ¨¡å¼è·å–å“åº”
        response = requests.get(test_url, headers=HEADERS, timeout=timeout, stream=True)
        
        if response.status_code != 200:
            response.close()
            return False, None
        
        # è¯»å–æ•°æ®å—ï¼Œè®¡ç®—é€Ÿåº¦
        for chunk in response.iter_content(chunk_size=8192):
            if chunk:
                chunks.append(len(chunk))
                downloaded_size += len(chunk)
                
                # å¦‚æœå·²ç»ä¸‹è½½äº†è¶³å¤Ÿçš„æ•°æ®æˆ–è€…è¶…è¿‡äº†æµ‹è¯•æ—¶é—´ï¼Œåœæ­¢æµ‹è¯•
                if downloaded_size >= SPEED_TEST_CONFIG['min_download_size'] or \
                   time.time() - start_time >= SPEED_TEST_CONFIG['test_duration']:
                    break
        
        response.close()
        
        # è®¡ç®—é€Ÿåº¦ï¼ˆKB/sï¼‰
        if downloaded_size > 0:
            duration = time.time() - start_time
            speed_kbps = (downloaded_size / 1024) / duration
            
            # å¦‚æœæœ‰å¤šä¸ªæ•°æ®å—ï¼Œè®¡ç®—ç¨³å®šæ€§ï¼ˆæ ‡å‡†å·®ä¸å‡å€¼çš„æ¯”ç‡ï¼‰
            if len(chunks) > 1:
                chunk_speeds = [len(c) / (time.time() - start_time) * 1024 for c in chunks]
                stability = statistics.stdev(chunk_speeds) / statistics.mean(chunk_speeds) if len(chunk_speeds) > 1 else 0
            else:
                stability = 0
            
            return True, {
                'speed_kbps': speed_kbps,
                'downloaded_size': downloaded_size,
                'duration': duration,
                'stability': 1 - stability,  # ç¨³å®šæ€§å€¼ï¼Œ1è¡¨ç¤ºæœ€ç¨³å®š
                'test_url': test_url
            }
        else:
            return True, None
    except Exception as e:
        return False, None

# æ‰¹é‡æµ‹è¯•URLé€Ÿåº¦
def batch_test_urls(urls: List[str]) -> Dict[str, Dict]:
    """æ‰¹é‡æµ‹è¯•URLåˆ—è¡¨çš„é€Ÿåº¦"""
    results = {}
    
    print(f"\nğŸ“Š å¼€å§‹æµ‹é€Ÿï¼šå…±{len(urls)}ä¸ªURLéœ€è¦æµ‹è¯•")
    start_time = time.time()
    
    # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘æµ‹è¯•
    with concurrent.futures.ThreadPoolExecutor(max_workers=SPEED_TEST_CONFIG['max_workers']) as executor:
        future_to_url = {executor.submit(test_url_speed, url): url for url in urls}
        
        for i, future in enumerate(concurrent.futures.as_completed(future_to_url)):
            url = future_to_url[future]
            try:
                is_valid, speed_info = future.result()
                results[url] = {
                    'valid': is_valid,
                    'speed_info': speed_info
                }
                
                # æ˜¾ç¤ºè¿›åº¦
                if (i + 1) % 10 == 0 or i + 1 == len(urls):
                    print(f"  è¿›åº¦: {i + 1}/{len(urls)} URLå·²æµ‹è¯•")
            except Exception as e:
                results[url] = {
                    'valid': False,
                    'speed_info': None
                }
    
    total_time = time.time() - start_time
    print(f"âœ… æµ‹é€Ÿå®Œæˆï¼Œè€—æ—¶{total_time:.2f}ç§’")
    
    # ç»Ÿè®¡ç»“æœ
    valid_count = sum(1 for r in results.values() if r['valid'])
    speed_count = sum(1 for r in results.values() if r['valid'] and r['speed_info'])
    
    print(f"ğŸ“ˆ æµ‹é€Ÿç»Ÿè®¡ï¼š")
    print(f"  æœ‰æ•ˆURL: {valid_count}/{len(urls)}")
    print(f"  æˆåŠŸæµ‹é€Ÿ: {speed_count}/{len(urls)}")
    
    return results

# è¯»å–æ–‡ä»¶å†…å®¹
def read_file():
    try:
        with open(FILE_PATH, 'r', encoding='utf-8') as f:
            return f.readlines()
    except:
        sys.exit(1)

# å†™å…¥æ–‡ä»¶å†…å®¹
def write_file(lines):
    try:
        with open(FILE_PATH, 'w', encoding='utf-8') as f:
            f.writelines(lines)
        return True
    except:
        sys.exit(1)

# å¤„ç†4Ké¢‘é“URLï¼ŒéªŒè¯å¹¶æ›´æ–°
def process_uhd_channels(lines):
    """å¤„ç†4Ké¢‘é“åˆ—è¡¨ï¼ŒéªŒè¯URLæœ‰æ•ˆæ€§ï¼Œæµ‹è¯•é€Ÿåº¦å¹¶æ›´æ–°æ—¶é—´æˆ³"""
    processed_lines = []
    valid_channels = []
    github_sources_section = False
    channel_section = False
    urls_to_test = []
    url_mappings = {}
    
    # è§£ææ–‡ä»¶å†…å®¹ï¼Œæ”¶é›†éœ€è¦æµ‹è¯•çš„URL
    print("ğŸ” è§£ææ–‡ä»¶å†…å®¹ï¼Œæ”¶é›†é¢‘é“ä¿¡æ¯...")
    for line in lines:
        line = line.strip()
        
        # è·³è¿‡æ–‡ä»¶å¤´éƒ¨å’Œç©ºè¡Œ
        if not line or line.startswith('# æ›´æ–°æ—¶é—´') or line.startswith('# å…±åŒ…å«'):
            continue
        
        # å¤„ç†é¢‘é“è¡Œï¼ˆæ ¼å¼ï¼šé¢‘é“åç§°,URLï¼‰
        if ',' in line:
            parts = line.split(',')
            if len(parts) >= 2:
                channel_name = parts[0].strip()
                url = parts[1].strip()
                
                # å¦‚æœæ˜¯GitHub URLï¼Œæ·»åŠ å‰ç¼€
                if url.startswith('https://raw.githubusercontent.com/'):
                    test_url = f"https://ghfast.top/{url}"
                else:
                    test_url = url
                
                urls_to_test.append(test_url)
                url_mappings[test_url] = (channel_name, url)
        # å¤„ç†GitHubæºå»ºè®®ä¸­çš„URLè¡Œ
        elif line.startswith('https://raw.githubusercontent.com/'):
            test_url = f"https://ghfast.top/{line}"
            urls_to_test.append(test_url)
            url_mappings[test_url] = ("GitHubæºå»ºè®®", line)
        # å¤„ç†æ³¨é‡Šä¸­çš„URL
        elif '# ' in line and 'https://raw.githubusercontent.com/' in line:
            match = re.search(r'(# \d+\.) (https://raw\.githubusercontent\.com/.+)', line)
            if match:
                url = match.group(2)
                test_url = f"https://ghfast.top/{url}"
                urls_to_test.append(test_url)
                url_mappings[test_url] = ("GitHubæºå»ºè®®", url)
    
    # æµ‹è¯•URLé€Ÿåº¦
    speed_results = {}
    if SPEED_TEST_CONFIG['speed_test_enabled'] and urls_to_test:
        speed_results = batch_test_urls(urls_to_test)
    
    # æ·»åŠ æ–‡ä»¶å¤´éƒ¨ä¿¡æ¯
    processed_lines.append("# 4Kè¶…é«˜æ¸…ç›´æ’­æºåˆ—è¡¨\n")
    processed_lines.append(f"# æ›´æ–°æ—¶é—´: {time.strftime('%Y-%m-%d')}\n")
    processed_lines.append(f"# å…±åŒ…å« 0 ä¸ª4Kè¶…é«˜æ¸…é¢‘é“\n")
    processed_lines.append("# æµ‹é€Ÿç»“æœä¼šåœ¨æ–‡ä»¶ä¸­ä»¥æ³¨é‡Šå½¢å¼æ˜¾ç¤º\n")
    processed_lines.append("\n")
    
    # å¤„ç†æ–‡ä»¶å†…å®¹
    for line in lines:
        line = line.strip()
        
        # è·³è¿‡é‡å¤çš„æ–‡ä»¶å¤´éƒ¨
        if line in ['# 4Kè¶…é«˜æ¸…ç›´æ’­æºåˆ—è¡¨', '# æ›´æ–°æ—¶é—´: 2024-11-24', '# æ›´æ–°æ—¶é—´: 2025-11-29', '# å…±åŒ…å« 0 ä¸ª4Kè¶…é«˜æ¸…é¢‘é“']:
            continue
        
        # å¤„ç†ç©ºè¡Œ
        if not line:
            processed_lines.append('\n')
            continue
        
        # å¤„ç†æ³¨é‡Šè¡Œ
        if line.startswith('#'):
            # æ£€æŸ¥æ˜¯å¦è¿›å…¥GitHubæºå»ºè®®éƒ¨åˆ†
            if 'å»ºè®®æ·»åŠ åˆ°get_cgq_sources.py' in line or 'ä»¥ä¸‹æ˜¯' in line:
                github_sources_section = True
                channel_section = False
                processed_lines.append(line + '\n')
            # æ£€æŸ¥æ˜¯å¦è¿›å…¥4Kå¤®è§†é¢‘é“éƒ¨åˆ†
            elif line == '# 4Kå¤®è§†é¢‘é“':
                github_sources_section = False
                channel_section = True
                processed_lines.append('# 4Kå¤®è§†é¢‘é“\n')
                processed_lines.append('\n')
            # å¤„ç†GitHubæºå»ºè®®ä¸­çš„URLæ³¨é‡Šè¡Œ
            elif '# ' in line and 'https://raw.githubusercontent.com/' in line:
                match = re.search(r'(# \d+\.) (https://raw\.githubusercontent\.com/.+)', line)
                if match:
                    prefix = match.group(1)
                    url = match.group(2)
                    proxied_url = f"https://ghfast.top/{url}"
                    
                    # æ·»åŠ å¸¦æœ‰å‰ç¼€çš„URL
                    processed_lines.append(f"{prefix} {proxied_url}\n")
                    
                    # æ·»åŠ æµ‹é€Ÿç»“æœæ³¨é‡Š
                    if SPEED_TEST_CONFIG['speed_test_enabled'] and proxied_url in speed_results:
                        result = speed_results[proxied_url]
                        if result['valid']:
                            valid_channels.append(("GitHubæºå»ºè®®", proxied_url))
                            if result['speed_info']:
                                speed_kbps = result['speed_info']['speed_kbps']
                                stability = result['speed_info']['stability']
                                processed_lines.append(f"#   é€Ÿåº¦: {speed_kbps:.1f} KB/s, ç¨³å®šæ€§: {stability:.2f}\n")
                            else:
                                processed_lines.append(f"#   çŠ¶æ€: æœ‰æ•ˆï¼Œä½†æ— æ³•æµ‹é€Ÿ\n")
                        else:
                            processed_lines.append(f"#   çŠ¶æ€: æ— æ•ˆ\n")
                else:
                    processed_lines.append(line + '\n')
            # ä¿ç•™å…¶ä»–æ³¨é‡Šè¡Œ
            else:
                processed_lines.append(line + '\n')
        # å¤„ç†é¢‘é“è¡Œï¼ˆæ ¼å¼ï¼šé¢‘é“åç§°,URLï¼‰
        elif ',' in line and channel_section:
            parts = line.split(',')
            if len(parts) >= 2:
                channel_name = parts[0].strip()
                url = parts[1].strip()
                
                # å¦‚æœæ˜¯GitHub URLï¼Œæ·»åŠ å‰ç¼€
                if url.startswith('https://raw.githubusercontent.com/'):
                    proxied_url = f"https://ghfast.top/{url}"
                    test_url = proxied_url
                else:
                    proxied_url = url
                    test_url = url
                
                # æ£€æŸ¥URLæ˜¯å¦æœ‰æ•ˆ
                is_valid = False
                speed_comment = ""
                
                if SPEED_TEST_CONFIG['speed_test_enabled'] and test_url in speed_results:
                    result = speed_results[test_url]
                    is_valid = result['valid']
                    
                    if is_valid:
                        valid_channels.append((channel_name, proxied_url))
                        if result['speed_info']:
                            speed_kbps = result['speed_info']['speed_kbps']
                            stability = result['speed_info']['stability']
                            speed_comment = f"# é€Ÿåº¦: {speed_kbps:.1f} KB/s, ç¨³å®šæ€§: {stability:.2f} \n"
                        else:
                            speed_comment = f"# çŠ¶æ€: æœ‰æ•ˆï¼Œä½†æ— æ³•æµ‹é€Ÿ \n"
                else:
                    # å¦‚æœæ²¡æœ‰æµ‹é€Ÿç»“æœï¼Œä½¿ç”¨ä¼ ç»ŸéªŒè¯æ–¹æ³•
                    is_valid = test_url_speed(test_url)[0]
                    if is_valid:
                        valid_channels.append((channel_name, proxied_url))
                
                if is_valid:
                    processed_lines.append(f"{channel_name},{proxied_url}\n")
                    if speed_comment:
                        processed_lines.append(speed_comment)
        # å¤„ç†GitHubæºå»ºè®®ä¸­çš„ç›´æ¥URLè¡Œ
        elif line.startswith('https://raw.githubusercontent.com/'):
            proxied_url = f"https://ghfast.top/{line}"
            
            # æ·»åŠ å¸¦æœ‰å‰ç¼€çš„URL
            processed_lines.append(proxied_url + '\n')
            
            # æ·»åŠ æµ‹é€Ÿç»“æœæ³¨é‡Š
            if SPEED_TEST_CONFIG['speed_test_enabled'] and proxied_url in speed_results:
                result = speed_results[proxied_url]
                if result['valid']:
                    valid_channels.append(("GitHubæºå»ºè®®", proxied_url))
                    if result['speed_info']:
                        speed_kbps = result['speed_info']['speed_kbps']
                        stability = result['speed_info']['stability']
                        processed_lines.append(f"# é€Ÿåº¦: {speed_kbps:.1f} KB/s, ç¨³å®šæ€§: {stability:.2f}\n")
                    else:
                        processed_lines.append(f"# çŠ¶æ€: æœ‰æ•ˆï¼Œä½†æ— æ³•æµ‹é€Ÿ\n")
                else:
                    processed_lines.append(f"# çŠ¶æ€: æ— æ•ˆ\n")
        # ä¿ç•™å…¶ä»–è¡Œ
        else:
            processed_lines.append(line + '\n')
    
    # æ›´æ–°é¢‘é“æ•°é‡
    for i, line in enumerate(processed_lines):
        if line.startswith('# å…±åŒ…å«'):
            processed_lines[i] = f"# å…±åŒ…å« {len(valid_channels)} ä¸ª4Kè¶…é«˜æ¸…é¢‘é“\n"
            break
    
    # æŒ‰URLé€Ÿåº¦å¯¹é¢‘é“è¿›è¡Œæ’åºï¼ˆå¦‚æœå¯ç”¨äº†æµ‹é€Ÿï¼‰
    if SPEED_TEST_CONFIG['speed_test_enabled']:
        print("ğŸ”„ æ ¹æ®æµ‹é€Ÿç»“æœå¯¹é¢‘é“è¿›è¡Œæ’åº...")
        # è¿™é‡Œå¯ä»¥å®ç°æ›´å¤æ‚çš„æ’åºé€»è¾‘
        # ä¸ºç®€åŒ–å®ç°ï¼Œæˆ‘ä»¬ä¿æŒåŸæœ‰çš„é¡ºåºï¼Œä½†åœ¨æ³¨é‡Šä¸­æ˜¾ç¤ºé€Ÿåº¦ä¿¡æ¯
    
    return processed_lines

# ä¸»å‡½æ•°
def main():
    print("ğŸš€ 4Kç›´æ’­æºå¤„ç†å·¥å…·å¯åŠ¨")
    print(f"ğŸ“‹ æµ‹é€Ÿè®¾ç½®ï¼š")
    print(f"  - å¯ç”¨çŠ¶æ€: {'âœ… å·²å¯ç”¨' if SPEED_TEST_CONFIG['speed_test_enabled'] else 'âŒ å·²ç¦ç”¨'}")
    print(f"  - è¶…æ—¶æ—¶é—´: {SPEED_TEST_CONFIG['timeout']}ç§’")
    print(f"  - æµ‹è¯•æ—¶é•¿: {SPEED_TEST_CONFIG['test_duration']}ç§’")
    print(f"  - å¹¶å‘çº¿ç¨‹: {SPEED_TEST_CONFIG['max_workers']}")
    print(f"  - æœ€å°ä¸‹è½½: {SPEED_TEST_CONFIG['min_download_size'] / 1024:.1f}KB")
    
    start_time = time.time()
    
    # è¯»å–æ–‡ä»¶å†…å®¹
    print("\nğŸ“ è¯»å–4Ké¢‘é“æ–‡ä»¶...")
    lines = read_file()
    
    # å¤„ç†4Ké¢‘é“ï¼ŒéªŒè¯URLå¹¶æ›´æ–°æ—¶é—´æˆ³
    print("ğŸ”§ å¼€å§‹å¤„ç†4Ké¢‘é“...")
    processed_lines = process_uhd_channels(lines)
    
    # å†™å…¥å¤„ç†åçš„å†…å®¹
    print("ğŸ’¾ ä¿å­˜å¤„ç†ç»“æœ...")
    write_file(processed_lines)
    
    total_time = time.time() - start_time
    print(f"\nğŸ† 4Ké¢‘é“å¤„ç†ä»»åŠ¡å®Œæˆï¼")
    print(f"â±ï¸  æ€»è€—æ—¶: {total_time:.2f}ç§’")
    
    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    print("\nğŸ“Š å¤„ç†ç»Ÿè®¡ï¼š")
    valid_count = 0
    for line in processed_lines:
        if ',' in line and not line.startswith('#'):
            valid_count += 1
    print(f"  - æœ‰æ•ˆ4Ké¢‘é“: {valid_count}ä¸ª")
    print(f"  - æ›´æ–°æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}")
    print("\nğŸ’¡ æç¤ºï¼šæµ‹é€Ÿç»“æœå·²åœ¨æ–‡ä»¶ä¸­ä»¥æ³¨é‡Šå½¢å¼æ˜¾ç¤ºï¼Œæ ¼å¼ä¸º'# é€Ÿåº¦: XX.X KB/s, ç¨³å®šæ€§: X.XX'")

# æµ‹è¯•è„šæœ¬
if __name__ == "__main__":
    main()
