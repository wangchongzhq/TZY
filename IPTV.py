#!/usr/bin/env python3
"""
IPTVç›´æ’­æºè‡ªåŠ¨ç”Ÿæˆå·¥å…· - ç®€åŒ–ç‰ˆ
åŠŸèƒ½ï¼šä»å¤šä¸ªæ¥æºè·å–IPTVç›´æ’­æºå¹¶ç”ŸæˆM3Uæ–‡ä»¶
æ ¸å¿ƒç‰¹æ€§ï¼šæ ‡å‡†é¢‘é“åˆ†ç±» + é¢‘é“åˆ—è¡¨åˆ«å
å€Ÿé‰´fetch.pyçš„ä¼˜ç§€è§£æé€»è¾‘å’Œé”™è¯¯å¤„ç†
"""

import os
import re
import sys
import json
import requests
import logging
import time
import tempfile
import multiprocessing
from concurrent.futures import ThreadPoolExecutor, as_completed
from collections import OrderedDict, defaultdict
from datetime import datetime, timezone, timedelta

# å¯¼å…¥é…ç½®
try:
    import config
except ImportError:
    print("é”™è¯¯: æ‰¾ä¸åˆ°config.pyé…ç½®æ–‡ä»¶")
    sys.exit(1)

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('iptv_update.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# è¯·æ±‚å¤´è®¾ç½®
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
}

def parse_template(template_file):
    """è§£æé¢‘é“æ¨¡æ¿æ–‡ä»¶ - å€Ÿé‰´fetch.pyçš„ç®€æ´é€»è¾‘"""
    template_channels = OrderedDict()
    current_category = None

    try:
        with open(template_file, "r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith("#"):
                    if "#genre#" in line:
                        current_category = line.split(",")[0].strip()
                        template_channels[current_category] = []
                        logger.debug(f"è§£æåˆ†ç±»: {current_category}")
                    elif current_category:
                        channel_name = line.split(",")[0].strip()
                        template_channels[current_category].append(channel_name)
                        logger.debug(f"è§£æé¢‘é“: {channel_name}")

        logger.info(f"æ¨¡æ¿è§£æå®Œæˆ: {len(template_channels)} ä¸ªåˆ†ç±»")
        return template_channels
        
    except Exception as e:
        logger.error(f"è§£ææ¨¡æ¿æ–‡ä»¶å¤±è´¥ {template_file}: {e}")
        return OrderedDict()

def fetch_channels(url):
    """è·å–å¹¶è§£ææºæ•°æ® - å€Ÿé‰´fetch.pyçš„é€»è¾‘"""
    channels = OrderedDict()

    try:
        logger.info(f"è·å–æºæ•°æ®: {url}")
        response = requests.get(url, headers=HEADERS, timeout=30)
        response.raise_for_status()
        response.encoding = 'utf-8'
        lines = response.text.split("\n")
        current_category = None
        
        # è‡ªåŠ¨æ£€æµ‹æ ¼å¼
        is_m3u = any("#EXTINF" in line for line in lines[:15])
        source_type = "m3u" if is_m3u else "txt"
        logger.info(f"æ£€æµ‹åˆ°æ ¼å¼: {source_type}")

        if is_m3u:
            # è§£æM3Uæ ¼å¼
            for line in lines:
                line = line.strip()
                if line.startswith("#EXTINF"):
                    match = re.search(r'group-title="(.*?)",(.*)', line)
                    if match:
                        current_category = match.group(1).strip()
                        channel_name = match.group(2).strip()
                        if current_category not in channels:
                            channels[current_category] = []
                elif line and not line.startswith("#"):
                    channel_url = line.strip()
                    if current_category and channel_name:
                        channels[current_category].append((channel_name, channel_url))
        else:
            # è§£æTXTæ ¼å¼
            for line in lines:
                line = line.strip()
                if "#genre#" in line:
                    current_category = line.split(",")[0].strip()
                    channels[current_category] = []
                elif current_category:
                    match = re.match(r"^(.*?),(.*?)$", line)
                    if match:
                        channel_name = match.group(1).strip()
                        channel_url = match.group(2).strip()
                        channels[current_category].append((channel_name, channel_url))
                    elif line:
                        channels[current_category].append((line, ''))

        if channels:
            categories = ", ".join(channels.keys())
            logger.info(f"è·å–æˆåŠŸâœ…ï¼ŒåŒ…å«åˆ†ç±»: {categories}")
        else:
            logger.warning(f"è·å–å¤±è´¥âŒ: æœªèƒ½è§£æåˆ°é¢‘é“æ•°æ®")

    except requests.RequestException as e:
        logger.error(f"è·å–å¤±è´¥âŒ {url}: {e}")
    except Exception as e:
        logger.error(f"è§£æå¤±è´¥âŒ {url}: {e}")

    return channels

def match_channels(template_channels, all_channels, channel_mapping=None):
    """åŒ¹é…é¢‘é“ - æ”¯æŒåˆ«ååŒ¹é…"""
    matched_channels = OrderedDict()
    channel_mapping = channel_mapping or {}

    logger.info(f"å¼€å§‹é¢‘é“åŒ¹é…: æ¨¡æ¿{len(template_channels)}åˆ†ç±», æº{len(all_channels)}åˆ†ç±»")

    for category, channel_list in template_channels.items():
        matched_channels[category] = OrderedDict()
        logger.debug(f"å¤„ç†åˆ†ç±»: {category} ({len(channel_list)}é¢‘é“)")
        
        for channel_name in channel_list:
            # 1. ç²¾ç¡®åŒ¹é…
            exact_matches = []
            for online_category, online_channel_list in all_channels.items():
                for online_channel_name, online_channel_url in online_channel_list:
                    if channel_name == online_channel_name:
                        exact_matches.append(online_channel_url)
            
            if exact_matches:
                matched_channels[category][channel_name] = exact_matches
                logger.debug(f"ç²¾ç¡®åŒ¹é…: {channel_name} -> {len(exact_matches)}ä¸ªURL")
            
            # 2. åˆ«ååŒ¹é…
            if channel_name in channel_mapping:
                alias_matches = []
                for alias in channel_mapping[channel_name]:
                    for online_category, online_channel_list in all_channels.items():
                        for online_channel_name, online_channel_url in online_channel_list:
                            if alias == online_channel_name:
                                alias_matches.append(online_channel_url)
                
                if alias_matches:
                    if channel_name in matched_channels:
                        matched_channels[channel_name].extend(alias_matches)
                    else:
                        matched_channels[category][channel_name] = alias_matches
                    logger.debug(f"åˆ«ååŒ¹é…: {channel_name} -> {len(alias_matches)}ä¸ªURL")

    # ç»Ÿè®¡åŒ¹é…ç»“æœ
    total_matched = sum(len(channels) for channels in matched_channels.values() 
                      for channels in channels.values())
    logger.info(f"é¢‘é“åŒ¹é…å®Œæˆ: æ€»è®¡åŒ¹é…{total_matched}ä¸ªURL")

    return matched_channels

def generate_output(matched_channels, template_channels, output_file_m3u="jieguo.m3u", output_file_txt="jieguo.txt"):
    """ç”Ÿæˆè¾“å‡ºæ–‡ä»¶ - ç®€åŒ–çš„ç‰ˆæœ¬ï¼Œåªä¿ç•™æ ¸å¿ƒåŠŸèƒ½"""
    
    logger.info(f"å¼€å§‹ç”Ÿæˆè¾“å‡ºæ–‡ä»¶: {output_file_m3u}, {output_file_txt}")
    
    with open(output_file_m3u, "w", encoding="utf-8") as f_m3u, \
         open(output_file_txt, "w", encoding="utf-8") as f_txt:
        
        # å†™å…¥M3Uå¤´éƒ¨
        f_m3u.write("#EXTM3U\n")
        
        # å¤„ç†æ¯ä¸ªåˆ†ç±»
        for category, channel_list in template_channels.items():
            if category in matched_channels:
                f_txt.write(f"{category},#genre#\n")
                
                for channel_name in channel_list:
                    if channel_name in matched_channels[category]:
                        urls = matched_channels[category][channel_name]
                        
                        for url in urls:
                            # å†™å…¥M3U
                            logo_url = f"https://gcore.jsdelivr.net/gh/yuanzl77/TVlogo@master/png/{channel_name}.png"
                            f_m3u.write(f'#EXTINF:-1 tvg-name="{channel_name}" tvg-logo="{logo_url}" group-title="{category}",{channel_name}\n')
                            f_m3u.write(f"{url}\n")
                            
                            # å†™å…¥TXT
                            f_txt.write(f"{channel_name},{url}\n")
                
                f_txt.write("\n")  # åˆ†ç±»é—´ç©ºè¡Œ
    
    logger.info(f"è¾“å‡ºæ–‡ä»¶ç”Ÿæˆå®Œæˆ")

def main():
    """ä¸»å‡½æ•° - ç®€åŒ–ç‰ˆæœ¬"""
    print("ğŸš€ IPTVç›´æ’­æºè‡ªåŠ¨ç”Ÿæˆå·¥å…·")
    print(f"ğŸ“… è¿è¡Œæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 50)
    
    # è¯»å–é…ç½®
    config = {}
    try:
        if os.path.exists('iptv_config.json'):
            with open('iptv_config.json', 'r', encoding='utf-8') as f:
                config = json.load(f)
            logger.info("âœ… é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ")
    except Exception as e:
        logger.error(f"âŒ é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
        config = {}
    
    # è·å–æºURLåˆ—è¡¨
    source_urls = getattr(config, 'source_urls', config.get('sources', {}).get('default', []))
    if not source_urls:
        try:
            source_urls = config.source_urls
        except AttributeError:
            source_urls = []
    
    if not source_urls:
        logger.error("âŒ æœªæ‰¾åˆ°æºURLé…ç½®")
        return
    
    logger.info(f"ğŸ”§ ä½¿ç”¨æ ‡å‡†é¢‘é“åˆ†ç±» + åˆ«ååŒ¹é…æ¶æ„")
    
    # å¤„ç†æ‰€æœ‰æº
    all_channels = OrderedDict()
    for url in source_urls:
        channels = fetch_channels(url)
        for category, channel_list in channels.items():
            if category not in all_channels:
                all_channels[category] = []
            all_channels[category].extend(channel_list)
    
    # è·å–é¢‘é“åˆ«åæ˜ å°„
    channel_mapping = getattr(config, 'channel_mapping', {})
    
    # ä½¿ç”¨é»˜è®¤æ¨¡æ¿ç»“æ„
    template_channels = OrderedDict()
    # è¿™é‡Œå¯ä»¥æ·»åŠ æ ‡å‡†é¢‘é“åˆ†ç±»ï¼Œæˆ–è€…ä»æ¨¡æ¿æ–‡ä»¶è¯»å–
    
    # å¦‚æœæ²¡æœ‰æ¨¡æ¿æ–‡ä»¶ï¼Œä½¿ç”¨è·å–åˆ°çš„æ‰€æœ‰é¢‘é“
    if not template_channels:
        logger.info("æœªæ‰¾åˆ°æ¨¡æ¿æ–‡ä»¶ï¼Œä½¿ç”¨è·å–åˆ°çš„é¢‘é“ç»“æ„")
        template_channels = OrderedDict()
        for category, channel_list in all_channels.items():
            template_channels[category] = []
            seen_channels = set()
            for channel_name, url in channel_list:
                if channel_name not in seen_channels:
                    template_channels[category].append(channel_name)
                    seen_channels.add(channel_name)
    
    # åŒ¹é…é¢‘é“
    matched_channels = match_channels(template_channels, all_channels, channel_mapping)
    
    # ç”Ÿæˆè¾“å‡º
    output_m3u = config.get('output', {}).get('m3u_file', 'jieguo.m3u')
    output_txt = config.get('output', {}).get('txt_file', 'jieguo.txt')
    
    generate_output(matched_channels, template_channels, output_m3u, output_txt)
    
    logger.info("ğŸ‰ IPTVæºç”Ÿæˆå®Œæˆ")

if __name__ == "__main__":
    main()
DEFAULT_CONFIG = {
    "sources": {
        "default": [],  # ä»unified_sourceså¯¼å…¥ï¼Œå¯åœ¨é…ç½®æ–‡ä»¶ä¸­è¦†ç›–
        "local": [],    # æœ¬åœ°ç›´æ’­æºæ–‡ä»¶åˆ—è¡¨
        "custom": []    # ç”¨æˆ·è‡ªå®šä¹‰ç›´æ’­æºURLåˆ—è¡¨
    },
    "template": {
        "enabled": True,     # å¯ç”¨æ¨¡æ¿é©±åŠ¨å¤„ç†
        "file": "channels_template.txt",  # é¢‘é“æ¨¡æ¿æ–‡ä»¶è·¯å¾„
        "preserve_order": True,  # ä¿ç•™åŸæ¨¡æ¿ä¸­çš„é¢‘é“é¡ºåº
        "use_alias_matching": True  # å¯ç”¨åˆ«ååŒ¹é…
    },
    "filter": {
        "resolution": True,    # å¼€å¯åˆ†è¾¨ç‡è¿‡æ»¤
        "min_resolution": [1920, 1080],  # æœ€ä½åˆ†è¾¨ç‡è¦æ±‚
        "only_4k": False       # æ˜¯å¦åªè·å–4Ké¢‘é“
    },
    "url_testing": {
        "enable": False,   # ç¦ç”¨URLæœ‰æ•ˆæ€§æµ‹è¯•ä»¥é¿å…è¶…æ—¶ - 2026-01-01ä¼˜åŒ–
        "timeout": 3,      # URLæµ‹è¯•è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰- å¢åŠ åˆ°3ç§’
        "retries": 0,      # URLæµ‹è¯•é‡è¯•æ¬¡æ•°
        "workers": 8      # URLæµ‹è¯•å¹¶å‘æ•° - é™ä½åˆ°8ä¸ªçº¿ç¨‹é¿å…ç½‘ç»œå‹åŠ›
    },
    "network": {
        "ip_version_priority": "ipv4",  # IPç‰ˆæœ¬ä¼˜å…ˆçº§: ipv4, ipv6, auto
        "url_blacklist": [],            # URLé»‘åå•
        "enable_ipv6": True,           # å¯ç”¨IPv6æ”¯æŒ
        "timeout": 30,                 # ç½‘ç»œè¯·æ±‚è¶…æ—¶
        "retries": 3                   # ç½‘ç»œè¯·æ±‚é‡è¯•æ¬¡æ•°
    },
    "matching": {
        "channel_mapping": {},         # é¢‘é“åˆ«åæ˜ å°„è¡¨
        "enable_fuzzy_match": True,    # å¯ç”¨æ¨¡ç³ŠåŒ¹é…
        "fuzzy_threshold": 0.8,        # æ¨¡ç³ŠåŒ¹é…é˜ˆå€¼
        "enable_aliases": True,        # å¯ç”¨åˆ«ååŒ¹é…
        "case_sensitive": False        # é¢‘é“åç§°åŒ¹é…æ˜¯å¦åŒºåˆ†å¤§å°å†™
    },
    "cache": {
        "expiry_time": 3600,  # ç¼“å­˜æœ‰æ•ˆæœŸï¼ˆç§’ï¼‰
        "file": "source_cache.json"  # ç¼“å­˜æ–‡ä»¶è·¯å¾„
    },
    "output": {
        "m3u_file": "jieguo.m3u",  # M3Uè¾“å‡ºæ–‡ä»¶
        "txt_file": "jieguo.txt",   # TXTè¾“å‡ºæ–‡ä»¶
        "include_invalid": True,    # åœ¨è¾“å‡ºä¸­åŒ…å«æ— æ•ˆé¢‘é“
        "separate_valid_invalid": False,  # åˆ†åˆ«ä¿å­˜æœ‰æ•ˆå’Œæ— æ•ˆé¢‘é“
        "preserve_categories": True  # ä¿ç•™é¢‘é“åˆ†ç±»ç»“æ„
    },
    "logging": {
        "level": "INFO",            # æ—¥å¿—çº§åˆ«
        "file": "iptv_update.log",  # æ—¥å¿—æ–‡ä»¶
        "enable_console": True,     # å¯ç”¨æ§åˆ¶å°è¾“å‡º
        "enable_file": True         # å¯ç”¨æ–‡ä»¶è¾“å‡º
    }
}

# é…ç½®æ–‡ä»¶è·¯å¾„
CONFIG_FILE = "iptv_config.json"

# ä»ç»Ÿä¸€æ’­æ”¾æºæ–‡ä»¶å¯¼å…¥
try:
    from unified_sources import UNIFIED_SOURCES
    # å°†UNIFIED_SOURCESè®¾ç½®ä¸ºé»˜è®¤ç›´æ’­æº
    DEFAULT_CONFIG["sources"]["default"] = UNIFIED_SOURCES
except ImportError:
    print("è­¦å‘Š: æ— æ³•å¯¼å…¥unified_sourcesæ¨¡å—ï¼Œé»˜è®¤ç›´æ’­æºä¸ºç©º")

# å…¨å±€é…ç½®å˜é‡
config = DEFAULT_CONFIG.copy()

# ç›´æ’­æºå†…å®¹ç¼“å­˜é…ç½®
import hashlib

# ç¼“å­˜å­—å…¸ï¼Œæ ¼å¼ï¼š{url: (cached_time, content, etag, last_modified)}
source_cache = {}

# ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„ç¼“å­˜è®¾ç½®
CACHE_FILE = config["cache"]["file"]
cache_expiry_time = config["cache"]["expiry_time"]

# åˆ›å»ºå…¨å±€Sessionå¯¹è±¡ä»¥æé«˜è¯·æ±‚æ€§èƒ½
session = requests.Session()
session.headers.update(HEADERS)
# ä½¿ç”¨é…ç½®ä¸­çš„å¹¶å‘æ•°
test_workers = config["url_testing"]["workers"]
session.mount('http://', requests.adapters.HTTPAdapter(pool_connections=100, pool_maxsize=test_workers, max_retries=0))
session.mount('https://', requests.adapters.HTTPAdapter(pool_connections=100, pool_maxsize=test_workers, max_retries=0))

# ä¿å­˜ç¼“å­˜åˆ°æ–‡ä»¶
def save_cache():
    """å°†ç¼“å­˜ä¿å­˜åˆ°æ–‡ä»¶"""
    try:
        with open(CACHE_FILE, 'w', encoding='utf-8') as f:
            # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„æ ¼å¼
            serializable_cache = {}
            for url, (cached_time, content, etag, last_modified) in source_cache.items():
                serializable_cache[url] = {
                    'cached_time': cached_time,
                    'content': content,
                    'etag': etag,
                    'last_modified': last_modified
                }
            json.dump(serializable_cache, f, ensure_ascii=False, indent=2)
        return True
    except (IOError, OSError) as e:
        print(f"ä¿å­˜ç¼“å­˜å¤±è´¥: æ–‡ä»¶æ“ä½œé”™è¯¯ - {e}")
        return False
    except (ValueError, TypeError) as e:
        print(f"ä¿å­˜ç¼“å­˜å¤±è´¥: æ•°æ®æ ¼å¼é”™è¯¯ - {e}")
        return False
    except Exception as e:
        print(f"ä¿å­˜ç¼“å­˜å¤±è´¥: æœªçŸ¥é”™è¯¯ - {e}")
        return False

# ä»æ–‡ä»¶åŠ è½½ç¼“å­˜
def load_cache():
    """ä»æ–‡ä»¶åŠ è½½ç¼“å­˜"""
    global source_cache
    try:
        if os.path.exists(CACHE_FILE):
            with open(CACHE_FILE, 'r', encoding='utf-8') as f:
                serializable_cache = json.load(f)
                # è½¬æ¢å›åŸå§‹æ ¼å¼
                source_cache = {}
                for url, data in serializable_cache.items():
                    source_cache[url] = (
                        data['cached_time'],
                        data['content'],
                        data.get('etag'),
                        data.get('last_modified')
                    )
            print(f"âœ… ä»ç¼“å­˜æ–‡ä»¶åŠ è½½äº† {len(source_cache)} ä¸ªç¼“å­˜æ¡ç›®")
        return True
    except (IOError, OSError) as e:
        print(f"åŠ è½½ç¼“å­˜å¤±è´¥: æ–‡ä»¶æ“ä½œé”™è¯¯ - {e}")
        source_cache = {}
        return False
    except (ValueError, TypeError) as e:
        print(f"åŠ è½½ç¼“å­˜å¤±è´¥: æ•°æ®æ ¼å¼é”™è¯¯ - {e}")
        source_cache = {}
        return False
    except Exception as e:
        print(f"åŠ è½½ç¼“å­˜å¤±è´¥: æœªçŸ¥é”™è¯¯ - {e}")
        source_cache = {}
        return False

# è®¡ç®—å†…å®¹çš„MD5å“ˆå¸Œå€¼
def calculate_md5(content):
    """è®¡ç®—å­—ç¬¦ä¸²çš„MD5å“ˆå¸Œå€¼"""
    return hashlib.md5(content.encode('utf-8')).hexdigest()

# åŠ è½½é…ç½®æ–‡ä»¶
def load_config():
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    global config
    try:
        if os.path.exists(CONFIG_FILE):
            with open(CONFIG_FILE, 'r', encoding='utf-8') as f:
                user_config = json.load(f)
                
                # åˆå¹¶é…ç½®ï¼ˆç”¨æˆ·é…ç½®è¦†ç›–é»˜è®¤é…ç½®ï¼‰
                def merge_dicts(default, user):
                    for key, value in user.items():
                        if key in default and isinstance(default[key], dict) and isinstance(value, dict):
                            merge_dicts(default[key], value)
                        else:
                            default[key] = value
                    return default
                
                config = merge_dicts(config, user_config)
                print(f"âœ… ä»é…ç½®æ–‡ä»¶åŠ è½½äº†ç”¨æˆ·è®¾ç½®")
                
                # æ›´æ–°å…¨å±€å˜é‡
                update_global_vars_from_config()
        else:
            # åˆ›å»ºé»˜è®¤é…ç½®æ–‡ä»¶
            save_config()
            print(f"âœ… åˆ›å»ºäº†é»˜è®¤é…ç½®æ–‡ä»¶: {CONFIG_FILE}")
        return True
    except (IOError, OSError) as e:
        print(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: æ–‡ä»¶æ“ä½œé”™è¯¯ - {e}")
        config = DEFAULT_CONFIG.copy()
        update_global_vars_from_config()
        return False
    except (ValueError, TypeError) as e:
        print(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: æ•°æ®æ ¼å¼é”™è¯¯ - {e}")
        config = DEFAULT_CONFIG.copy()
        update_global_vars_from_config()
        return False
    except Exception as e:
        print(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: æœªçŸ¥é”™è¯¯ - {e}")
        config = DEFAULT_CONFIG.copy()
        update_global_vars_from_config()
        return False

# ä¿å­˜é…ç½®æ–‡ä»¶
def save_config():
    """ä¿å­˜é…ç½®æ–‡ä»¶"""
    try:
        with open(CONFIG_FILE, 'w', encoding='utf-8') as f:
            json.dump(config, f, ensure_ascii=False, indent=2)
        return True
    except Exception as e:
        print(f"ä¿å­˜é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
        return False

# æ›´æ–°å…¨å±€å˜é‡
def update_global_vars_from_config():
    """ä»é…ç½®æ›´æ–°å…¨å±€å˜é‡"""
    global CACHE_FILE, cache_expiry_time
    
    # æ›´æ–°ç¼“å­˜è®¾ç½®
    CACHE_FILE = config["cache"]["file"]
    cache_expiry_time = config["cache"]["expiry_time"]

# æ¸…æ™°åº¦æ­£åˆ™è¡¨è¾¾å¼ - ç”¨äºè¯†åˆ«é«˜æ¸…çº¿è·¯
HD_PATTERNS = [
    # 4KåŠä»¥ä¸Š
    r'[48]k',
    r'2160[pdi]',
    r'uhd',
    r'è¶…é«˜æ¸…',
    r'4k',
    # 2K
    r'1440[pdi]',
    r'qhd',
    # 1080PåŠä»¥ä¸Š
    r'1080[pdi]',
    r'fhd',
    # å…¶ä»–é«˜æ¸…æ ‡è¯†
    r'é«˜æ¸…',
    r'è¶…æ¸…',
    r'hd',
    r'high.?definition',
    r'high.?def',
    # ç‰¹å®šçš„é«˜æ¸…æ ‡è¯†
    r'hdmi',
    r'è“å…‰',
    r'blue.?ray',
    r'hd.?live',
    # ç ç‡æ ‡è¯†
    r'[89]m',
    r'[1-9]\d+m',
    # ç‰¹å®šçš„URLå‚æ•°æ ‡è¯†
    r'quality=high',
    r'resolution=[1-9]\d{3}',
    r'hd=true',
    r'fhd=true'
]

HD_REGEX = re.compile('|'.join(HD_PATTERNS), re.IGNORECASE)

# é¢„ç¼–è¯‘å¸¸ç”¨æ­£åˆ™è¡¨è¾¾å¼
URL_REGEX = re.compile(r'(?:https?|udp|rtsp|rtmp|mms|rtp)://', re.IGNORECASE)

# é¢„ç¼–è¯‘åˆ†è¾¨ç‡å’Œè´¨é‡ç›¸å…³çš„æ­£åˆ™è¡¨è¾¾å¼
HIGH_DEF_PATTERNS = re.compile(r'(1080[pdi]|1440[pdi]|2160[pdi]|fhd|uhd|è¶…é«˜æ¸…)', re.IGNORECASE)
RES_PATTERNS = [
    re.compile(r'(\d{3,4})[pdi]'),  # å¦‚1080p, 2160i
    re.compile(r'(\d+)x(\d+)'),     # å¦‚1920x1080, 3840x2160
    re.compile(r'(\d+)_(\d+)'),     # å¦‚1920_1080
    re.compile(r'res=([1-9]\d+)'),       # å¦‚res=1080
    re.compile(r'resolution=([1-9]\d+)x?([1-9]\d+)'),  # å¦‚resolution=1920x1080
    re.compile(r'width=([1-9]\d+).*?height=([1-9]\d+)'),  # å¦‚width=1920 height=1080
]

# é¢„ç¼–è¯‘4Kç›¸å…³çš„æ­£åˆ™è¡¨è¾¾å¼
K4_PATTERNS = re.compile(r'(2160[pdi]|4k|8k|uhd|3840x2160|7680x4320|è¶…é«˜æ¸…)', re.IGNORECASE)
K4_RES_PATTERNS = [
    re.compile(r'(\d{3,4})[pdi]'),  # å¦‚2160p
    re.compile(r'(\d+)x(\d+)'),     # å¦‚3840x2160
]

# é¢„ç¼–è¯‘M3Ué¢‘é“æå–æ­£åˆ™è¡¨è¾¾å¼
M3U_CHANNEL_PATTERN = re.compile(r'#EXTINF:.*?tvg-name="([^"]*)".*?(?:group-title="([^"]*)")?,([^\n]+)\n(http[^\n]+)', re.DOTALL)

# é¢„ç¼–è¯‘å†…å®¹æ¸…ç†æ­£åˆ™è¡¨è¾¾å¼
CLEAN_CONTENT_PATTERN = re.compile(r'[\x00-\x08\x0b-\x0c\x0e-\x1f\x7f-\x9f\u20ac\ue000-\uf8ff]')

# è·å–URLåˆ—è¡¨
def get_urls_from_file(file_path):
    """ä»æ–‡ä»¶ä¸­è¯»å–URLåˆ—è¡¨"""
    urls = []
    if os.path.exists(file_path):
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                urls = [line.strip() for line in f if line.strip() and not line.startswith('#')]
        except Exception as e:
            print(f"è¯»å–URLæ–‡ä»¶æ—¶å‡ºé”™: {e}")
    return urls

# æµ‹è¯•é¢‘é“è¿‡æ»¤
def should_exclude_url(url):
    """æ£€æŸ¥æ˜¯å¦åº”è¯¥æ’é™¤ç‰¹å®šURLï¼ˆæµ‹è¯•é¢‘é“è¿‡æ»¤ï¼‰"""
    if not url:
        return True
    
    # æµ‹è¯•é¢‘é“è¿‡æ»¤ï¼šè¿‡æ»¤exampleã€demoã€sampleç­‰å…³é”®è¯
    test_patterns = ['example', 'demo', 'sample', 'samples']
    url_lower = url.lower()
    for pattern in test_patterns:
        if pattern in url_lower:
            return True
    
    # è¿‡æ»¤exampleåŸŸå
    if 'example.com' in url_lower or 'example.org' in url_lower:
        return True
    
    return False

# åˆ†è¾¨ç‡è¿‡æ»¤

def is_high_quality(line):
    """åˆ¤æ–­çº¿è·¯æ˜¯å¦ä¸ºé«˜æ¸…çº¿è·¯ï¼ˆ1080Pä»¥ä¸Šï¼‰"""
    # ä»lineä¸­æå–é¢‘é“åç§°å’ŒURL
    if 'http://' in line or 'https://' in line:
        # æå–URLä¹‹å‰çš„éƒ¨åˆ†ä½œä¸ºé¢‘é“åç§°
        channel_name = line.split('http://')[0].split('https://')[0].strip()
        # æå–URLéƒ¨åˆ†
        url_part = line[len(channel_name):].strip()
    else:
        channel_name = line.strip()
        url_part = ''
    
    # æ£€æŸ¥é¢‘é“åç§°ä¸­çš„é«˜æ¸…æ ‡è¯†
    if HIGH_DEF_PATTERNS.search(channel_name):
        return True
    
    # æ£€æŸ¥å…¶ä»–é«˜æ¸…æ ‡è¯†
    channel_name_lower = channel_name.lower()
    # é«˜æ¸…æ ‡è¯†åˆ—è¡¨
    hd_keywords = ['é«˜æ¸…', 'è¶…æ¸…', 'hd', 'high definition', 'high def']
    # ä½è´¨é‡æ ‡è¯†åˆ—è¡¨
    low_quality_keywords = ['360', '480', '576', 'æ ‡æ¸…', 'sd', 'low']
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«é«˜æ¸…æ ‡è¯†ä¸”ä¸åŒ…å«ä½è´¨é‡æ ‡è¯†
    if any(hd in channel_name_lower for hd in hd_keywords) and not any(low in channel_name_lower for low in low_quality_keywords):
        return True
    
    # åˆ†è¾¨ç‡è¿‡æ»¤ï¼šå¦‚æœå¼€å¯äº†åˆ†è¾¨ç‡è¿‡æ»¤ï¼Œæ£€æŸ¥æ˜¯å¦æ»¡è¶³æœ€å°åˆ†è¾¨ç‡è¦æ±‚
    if config["filter"]["resolution"]:
        # å¢å¼ºçš„åˆ†è¾¨ç‡æ£€æµ‹
        combined_text = channel_name + ' ' + url_part
        
        for pattern in RES_PATTERNS:
            res_match = pattern.search(combined_text)
            if res_match:
                try:
                    if len(res_match.groups()) == 1:
                        # å‚ç›´åˆ†è¾¨ç‡ï¼ˆå¦‚1080pï¼‰
                        res_value = int(res_match.group(1))
                        if res_value >= config["filter"]["min_resolution"][1]:
                            return True
                    elif len(res_match.groups()) == 2:
                        # å®Œæ•´åˆ†è¾¨ç‡ï¼ˆå¦‚1920x1080ï¼‰
                        width = int(res_match.group(1))
                        height = int(res_match.group(2))
                        if width >= config["filter"]["min_resolution"][0] and height >= config["filter"]["min_resolution"][1]:
                            return True
                except ValueError:
                    pass
    
    return False

def is_4k(channel_name, url):
    """åˆ¤æ–­é¢‘é“æ˜¯å¦ä¸º4Ké¢‘é“"""
    
    # æ£€æŸ¥é¢‘é“åç§°å’ŒURLä¸­çš„4Kæ ‡è¯†
    combined_text = channel_name + ' ' + url
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«4Kæ ‡è¯†
    if K4_PATTERNS.search(combined_text):
        return True
    
    # æ£€æŸ¥é¢‘é“åˆ†ç±»
    if get_simple_category(channel_name) == "4Ké¢‘é“":
        return True
    
    # æ£€æŸ¥åˆ†è¾¨ç‡
    for pattern in K4_RES_PATTERNS:
        res_match = pattern.search(combined_text)
        if res_match:
            try:
                if len(res_match.groups()) == 1:
                    # å‚ç›´åˆ†è¾¨ç‡ï¼ˆå¦‚2160pï¼‰
                    res_value = int(res_match.group(1))
                    if res_value >= 2160:
                        return True
                elif len(res_match.groups()) == 2:
                    # å®Œæ•´åˆ†è¾¨ç‡ï¼ˆå¦‚3840x2160ï¼‰
                    width = int(res_match.group(1))
                    height = int(res_match.group(2))
                    if width >= 3840 and height >= 2160:
                        return True
            except ValueError:
                pass
    
    return False

# æ£€æŸ¥URLæ˜¯å¦æœ‰æ•ˆ
def check_url(url, timeout=2, retries=0):
    """æ£€æŸ¥URLæ˜¯å¦å¯è®¿é—®ï¼Œæ”¯æŒé‡è¯•æœºåˆ¶"""
    # å…ˆæ£€æŸ¥URLæ ¼å¼æ˜¯å¦æ­£ç¡®
    if not URL_REGEX.match(url):
        return False
    
    # å¯¹äºéHTTP/HTTPSåè®®çš„URLï¼Œç›´æ¥è¿”å›Trueï¼ˆè¿™äº›åè®®æ— æ³•é€šè¿‡HEADè¯·æ±‚éªŒè¯ï¼‰
    if not url.startswith(('http://', 'https://')):
        return True
    
    for attempt in range(retries + 1):
        try:
            # ä½¿ç”¨HEADè¯·æ±‚ä»¥é¿å…ä¸‹è½½æ•´ä¸ªæ–‡ä»¶ï¼ˆä»…é€‚ç”¨äºHTTP/HTTPSï¼‰
            # æ·»åŠ Rangeå¤´å‡å°‘æµé‡ï¼Œåªè¯·æ±‚æ–‡ä»¶çš„ç¬¬ä¸€ä¸ªå­—èŠ‚
            response = session.head(
                url, 
                timeout=timeout, 
                allow_redirects=True,  # å…è®¸é‡å®šå‘ä»¥æé«˜æµ‹è¯•å‡†ç¡®æ€§
                headers={'Range': 'bytes=0-0'}  # è¯·æ±‚éƒ¨åˆ†å†…å®¹å‡å°‘æµé‡
            )
            # æ£€æŸ¥çŠ¶æ€ç ï¼Œ2xxè¡¨ç¤ºæˆåŠŸ
            return response.status_code < 400
        except requests.exceptions.RequestException as e:
            # å¦‚æœæ˜¯æœ€åä¸€æ¬¡å°è¯•æˆ–è€…æ˜¯ç‰¹å®šé”™è¯¯ï¼Œè¿”å›False
            if attempt == retries:
                return False

# æ ¼å¼åŒ–æ—¶é—´é—´éš”
def format_interval(seconds):
    """æ ¼å¼åŒ–æ—¶é—´é—´éš”"""
    if seconds < 60:
        return f"{seconds:.2f}ç§’"
    elif seconds < 3600:
        minutes, seconds = divmod(seconds, 60)
        return f"{int(minutes)}åˆ†{int(seconds)}ç§’"
    else:
        hours, remainder = divmod(seconds, 3600)
        minutes, seconds = divmod(remainder, 60)
        return f"{int(hours)}æ—¶{int(minutes)}åˆ†{int(seconds)}ç§’"

# è·å–IPåœ°å€
def get_ip_address():
    """è·å–æœ¬åœ°IPåœ°å€"""
    try:
        s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
        s.connect(('8.8.8.8', 80))
        ip = s.getsockname()[0]
        s.close()
        return ip
    except Exception as e:
        logger.error(f"è·å–IPåœ°å€å¤±è´¥: {e}")
        return "127.0.0.1"

# æ£€æŸ¥IPv6æ”¯æŒ
def check_ipv6_support():
    """æ£€æŸ¥ç³»ç»Ÿæ˜¯å¦æ”¯æŒIPv6"""
    try:
        socket.inet_pton(socket.AF_INET6, '::1')
        return True
    except Exception as e:
        logger.error(f"IPv6æ”¯æŒæ£€æŸ¥å¤±è´¥: {e}")
        return False

# ä»M3Uæ–‡ä»¶ä¸­æå–é¢‘é“ä¿¡æ¯
def extract_channels_from_m3u(content):
    """ä»M3Uå†…å®¹ä¸­æå–é¢‘é“ä¿¡æ¯"""
    channels = defaultdict(list)
    matches = re.findall(M3U_CHANNEL_PATTERN, content)
    
    for match in matches:
        tvg_name = match[0].strip() if match[0] else match[2].strip()
        channel_name = match[2].strip()
        url = match[3].strip()
        
        # æ£€æŸ¥é¢‘é“åæ˜¯å¦ä¸ºç©º
        if not channel_name:
            continue
        
        # æ£€æŸ¥é¢‘é“åæ˜¯å¦ä¸ºçº¯æ•°å­—
        if channel_name.isdigit():
            continue
        
        # è´­ç‰©é¢‘é“è¿‡æ»¤
        channel_name_lower = channel_name.lower()
        shopping_keywords = ['è´­ç‰©', 'å¯¼è´­', 'ç”µè§†è´­ç‰©']
        if any(keyword in channel_name_lower for keyword in shopping_keywords):
            continue
        
        # ç®€åŒ–å¤„ç†ï¼šç›´æ¥ä½¿ç”¨é¢‘é“åç§°ï¼Œç®€å•åˆ†ç±»
        category = get_simple_category(channel_name)
        channels[category].append((channel_name, url))
    
    return channels

# ç®€åŒ–é¢‘é“åˆ†ç±»
def get_simple_category(channel_name):
    """ç®€å•çš„é¢‘é“åˆ†ç±»"""
    name_lower = channel_name.lower()
    
    # 4Ké¢‘é“
    if any(keyword in name_lower for keyword in ['4k', '2160p', 'è¶…é«˜æ¸…', 'uhd']):
        return "4Ké¢‘é“"
    
    # å¤®è§†é¢‘é“
    if any(keyword in name_lower for keyword in ['å¤®è§†', 'cctv', 'cnn']):
        return "å¤®è§†é¢‘é“"
    
    # å«è§†é¢‘é“
    if any(keyword in name_lower for keyword in ['å«è§†', 'ä¸œæ–¹å«è§†', 'æ¹–å—å«è§†', 'æ±Ÿè‹å«è§†', 'æµ™æ±Ÿå«è§†']):
        return "å«è§†é¢‘é“"
    
    # æ¸¯æ¾³å°
    if any(keyword in name_lower for keyword in ['é¦™æ¸¯', 'å°æ¹¾', 'æ¾³é—¨', 'hktv', 'cti', 'ctv']):
        return "æ¸¯æ¾³é¢‘é“"
    
    # ç”µå½±é¢‘é“
    if any(keyword in name_lower for keyword in ['ç”µå½±', 'movie', 'cinema']):
        return "ç”µå½±é¢‘é“"
    
    # å„¿ç«¥é¢‘é“
    if any(keyword in name_lower for keyword in ['å„¿ç«¥', 'å°‘å„¿', 'åŠ¨ç”»', 'å¡é€š', 'kids']):
        return "å„¿ç«¥é¢‘é“"
    
    # ä½“è‚²é¢‘é“
    if any(keyword in name_lower for keyword in ['ä½“è‚²', 'sports', 'è¶³çƒ', 'ç¯®çƒ']):
        return "ä½“è‚²é¢‘é“"
    
    # ç»¼è‰ºé¢‘é“
    if any(keyword in name_lower for keyword in ['ç»¼è‰º', 'variety', 'å¨±ä¹']):
        return "ç»¼è‰ºé¢‘é“"
    
    # æ–°é—»é¢‘é“
    if any(keyword in name_lower for keyword in ['æ–°é—»', 'news', 'èµ„è®¯']):
        return "æ–°é—»é¢‘é“"
    
    # éŸ³ä¹é¢‘é“
    if any(keyword in name_lower for keyword in ['éŸ³ä¹', 'music', 'MTV']):
        return "éŸ³ä¹é¢‘é“"
    
    # é»˜è®¤åˆ†ç±»
    return "ç»¼åˆé¢‘é“"

# ä»URLè·å–M3Uå†…å®¹
def fetch_m3u_content(url, max_retries=3, timeout=120):
    """ä»URLæˆ–æœ¬åœ°æ–‡ä»¶è·å–M3Uå†…å®¹ï¼Œæ”¯æŒè¶…æ—¶ã€é‡è¯•æœºåˆ¶å’Œå¢é‡æ›´æ–°"""
    # å¤„ç†æœ¬åœ°æ–‡ä»¶è·¯å¾„
    if url.startswith('file://'):
        file_path = url[7:]  # ç§»é™¤file://å‰ç¼€
        try:
            print(f"æ­£åœ¨è¯»å–æœ¬åœ°æ–‡ä»¶: {file_path}")
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
                return content
        except Exception as e:
            print(f"è¯»å–æœ¬åœ°æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}")
            return None
    
    # æ£€æŸ¥ç¼“å­˜
    etag = None
    last_modified = None
    if url in source_cache:
        cached_time, cached_content, cached_etag, cached_last_modified = source_cache[url]
        if time.time() - cached_time < cache_expiry_time:
            print(f"æ­£åœ¨ä»ç¼“å­˜è·å–: {url}")
            return cached_content
        etag = cached_etag
        last_modified = cached_last_modified
    
    # ç¼“å­˜ä¸å­˜åœ¨æˆ–å·²è¿‡æœŸï¼Œå°è¯•å¢é‡æ›´æ–°
    headers = {}
    if etag:
        headers['If-None-Match'] = etag
    if last_modified:
        headers['If-Modified-Since'] = last_modified
    
    # å¤„ç†è¿œç¨‹URL
    for attempt in range(max_retries):
        try:
            # æ·»åŠ verify=Falseå‚æ•°æ¥è·³è¿‡SSLè¯ä¹¦éªŒè¯ï¼Œå¹¶ä½¿ç”¨è‡ªå®šä¹‰headers
            response = session.get(url, timeout=timeout, verify=False, headers=headers)
            
            if response.status_code == 304:
                # å†…å®¹æœªä¿®æ”¹ï¼Œä½¿ç”¨ç¼“å­˜å†…å®¹
                print(f"å†…å®¹æœªä¿®æ”¹ï¼Œä½¿ç”¨ç¼“å­˜: {url}")
                if url in source_cache:
                    cached_time, cached_content, cached_etag, cached_last_modified = source_cache[url]
                    # æ›´æ–°ç¼“å­˜æ—¶é—´
                    source_cache[url] = (time.time(), cached_content, cached_etag, cached_last_modified)
                    save_cache()
                    return cached_content
            
            response.raise_for_status()
            content = response.text
            
            # è·å–æ–°çš„ETagå’ŒLast-Modified
            new_etag = response.headers.get('ETag')
            new_last_modified = response.headers.get('Last-Modified')
            
            # æ£€æŸ¥å†…å®¹æ˜¯å¦æœ‰å˜åŒ–ï¼ˆå¦‚æœæœåŠ¡å™¨ä¸æ”¯æŒETag/Last-Modifiedï¼‰
            if url in source_cache:
                _, old_content, _, _ = source_cache[url]
                if calculate_md5(content) == calculate_md5(old_content):
                    print(f"å†…å®¹æœªå˜åŒ–ï¼Œæ›´æ–°ç¼“å­˜æ—¶é—´: {url}")
                    # å†…å®¹æœªå˜åŒ–ï¼Œæ›´æ–°ç¼“å­˜æ—¶é—´
                    source_cache[url] = (time.time(), old_content, new_etag, new_last_modified)
                    save_cache()
                    return old_content
            
            # æ›´æ–°ç¼“å­˜
            source_cache[url] = (time.time(), content, new_etag, new_last_modified)
            save_cache()
            
            print(f"è·å–æˆåŠŸ: {url}")
            return content
        except requests.exceptions.ConnectionError:
            # è¿æ¥é”™è¯¯ï¼Œé‡è¯•é—´éš”å¢åŠ 
            wait_time = 2 ** attempt  # æŒ‡æ•°é€€é¿
            print(f"è¿æ¥é”™è¯¯ï¼Œ{wait_time}ç§’åé‡è¯•...")
            time.sleep(wait_time)
        except requests.exceptions.Timeout:
            # è¶…æ—¶é”™è¯¯ï¼Œå¢åŠ è¶…æ—¶æ—¶é—´åé‡è¯•
            timeout = min(timeout * 1.5, 300)  # æœ€å¤§è¶…æ—¶5åˆ†é’Ÿ
            wait_time = 2 ** attempt
            print(f"è¯·æ±‚è¶…æ—¶ï¼Œ{wait_time}ç§’åé‡è¯•ï¼ˆæ–°è¶…æ—¶æ—¶é—´ï¼š{timeout}ç§’ï¼‰...")
            time.sleep(wait_time)
        except Exception as e:
            # å…¶ä»–é”™è¯¯
            print(f"è·å– {url} æ—¶å‡ºé”™: {e}")
            wait_time = 2 ** attempt if attempt < max_retries - 1 else 0
            if wait_time > 0:
                print(f"{wait_time}ç§’åé‡è¯•...")
                time.sleep(wait_time)
    return None



# ç”ŸæˆM3Uæ–‡ä»¶
def generate_m3u_file(channels, output_path):
    """ç”ŸæˆM3Uæ–‡ä»¶"""
    print(f"æ­£åœ¨ç”Ÿæˆ {output_path}...")
    
    print(f"ğŸ“ å¼€å§‹å†™å…¥æ–‡ä»¶: {output_path} æ—¶é—´: {datetime.now(timezone(timedelta(hours=8)))}")
    print(f"ğŸ“Š å†™å…¥å‰æ–‡ä»¶å¤§å°: {os.path.getsize(output_path) if os.path.exists(output_path) else 0} å­—èŠ‚")
    print(f"ğŸ“Š å†™å…¥å‰æ–‡ä»¶ä¿®æ”¹æ—¶é—´: {datetime.fromtimestamp(os.path.getmtime(output_path)) if os.path.exists(output_path) else 'ä¸å­˜åœ¨'}")
    
    with open(output_path, 'w', encoding='utf-8') as f:
        # å†™å…¥æ–‡ä»¶å¤´
        f.write("#EXTM3U x-tvg-url=\"https://kakaxi-1.github.io/IPTV/epg.xml\"\n")
        
        # å†™å…¥å½“å‰æ—¶é—´ä½œä¸ºæ ‡è®°ï¼ˆåŒ—äº¬æ—¶é—´UTC+8ï¼‰
        f.write(f"# ç”Ÿæˆæ—¶é—´: {datetime.now(timezone(timedelta(hours=8))).strftime('%Y-%m-%d %H:%M:%S.%f')}\n")
        
        # æŒ‰åˆ†ç±»é¡ºåºå†™å…¥é¢‘é“
        written_count = 0
        # å®šä¹‰æ ‡å‡†åˆ†ç±»é¡ºåº
        category_order = ["4Ké¢‘é“", "å¤®è§†é¢‘é“", "å«è§†é¢‘é“", "æ¸¯æ¾³é¢‘é“", "ç”µå½±é¢‘é“", "å„¿ç«¥é¢‘é“", "ä½“è‚²é¢‘é“", "ç»¼è‰ºé¢‘é“", "æ–°é—»é¢‘é“", "éŸ³ä¹é¢‘é“", "ç»¼åˆé¢‘é“"]
        
        for category in category_order:
            if category in channels:
                # å¯¹å½“å‰ç±»åˆ«çš„é¢‘é“æŒ‰åç§°å‡åºæ’åº
                sorted_channels = sorted(channels[category], key=lambda x: x[0])
                for channel_name, url in sorted_channels:
                    # å†™å…¥é¢‘é“ä¿¡æ¯
                    f.write(f"#EXTINF:-1 tvg-name=\"{channel_name}\" group-title=\"{category}\",{channel_name}\n")
                    f.write(f"{url}\n")
                    written_count += 1
    
    print(f"ğŸ“ å®Œæˆå†™å…¥æ–‡ä»¶: {output_path} æ—¶é—´: {datetime.now(timezone(timedelta(hours=8)))}")
    print(f"ğŸ“Š å†™å…¥åæ–‡ä»¶å¤§å°: {os.path.getsize(output_path)} å­—èŠ‚")
    print(f"ğŸ“Š å†™å…¥åæ–‡ä»¶ä¿®æ”¹æ—¶é—´: {datetime.fromtimestamp(os.path.getmtime(output_path))}")
    print(f"ğŸ“Š å®é™…å†™å…¥é¢‘é“æ•°: {written_count}")
    return True

# ç”ŸæˆTXTæ–‡ä»¶
def generate_txt_file(channels, output_path):
    """ç”ŸæˆTXTæ–‡ä»¶ï¼ˆå‚è€ƒBlackBird-Playerçš„result.txtæ ¼å¼ï¼‰"""
    print(f"æ­£åœ¨ç”Ÿæˆ {output_path}...")
    
    # æ›´æ–°æ—¶é—´æˆ³
    timestamp = datetime.now(timezone(timedelta(hours=8))).strftime('%Y-%m-%d %H:%M:%S')
    
    with open(output_path, 'w', encoding='utf-8') as f:
        # å†™å…¥æ›´æ–°æ—¶é—´æˆ³ï¼ˆå‚è€ƒBlackBird-Playeræ ¼å¼ï¼‰
        f.write("ğŸ•˜ï¸æ›´æ–°æ—¶é—´,#genre#\n")
        f.write(f"{timestamp}\n\n")
        
        # æŒ‰åˆ†ç±»é¡ºåºå†™å…¥é¢‘é“
        category_order = ["4Ké¢‘é“", "å¤®è§†é¢‘é“", "å«è§†é¢‘é“", "æ¸¯æ¾³é¢‘é“", "ç”µå½±é¢‘é“", "å„¿ç«¥é¢‘é“", "ä½“è‚²é¢‘é“", "ç»¼è‰ºé¢‘é“", "æ–°é—»é¢‘é“", "éŸ³ä¹é¢‘é“", "ç»¼åˆé¢‘é“"]
        
        for category in category_order:
            if category in channels and channels[category]:
                # å†™å…¥åˆ†ç»„æ ‡é¢˜ï¼Œä½¿ç”¨æ ¼å¼: åˆ†ç»„å,#genre#
                f.write(f"{category},#genre#\n")
                
                # å¯¹å½“å‰ç±»åˆ«çš„é¢‘é“æŒ‰åç§°å‡åºæ’åº
                sorted_channels = sorted(channels[category], key=lambda x: x[0])
                # å†™å…¥è¯¥åˆ†ç»„ä¸‹çš„æ‰€æœ‰é¢‘é“
                for channel_name, url in sorted_channels:
                    f.write(f"{channel_name},{url}\n")
                
                # åˆ†ç»„ä¹‹é—´æ·»åŠ ç©ºè¡Œ
                f.write("\n")
        
        # åœ¨æ–‡ä»¶æœ«å°¾æ·»åŠ è¯´æ˜è¡Œ
        f.write("\nè¯´æ˜,#genre#\n")
        
        # å†™å…¥æ–‡ä»¶å¤´æ³¨é‡Šåˆ°æ–‡ä»¶æœ«å°¾
        f.write(f"# IPTVç›´æ’­æºåˆ—è¡¨\n")
        f.write(f"# ç”Ÿæˆæ—¶é—´: {datetime.now(timezone(timedelta(hours=8))).strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write("# æ ¼å¼: é¢‘é“åç§°,æ’­æ”¾URL\n")
        f.write("# æŒ‰åˆ†ç»„æ’åˆ—\n")
        f.write("\n")
        
        # å†™å…¥é¢‘é“åˆ†ç±»è¯´æ˜
        f.write("# é¢‘é“åˆ†ç±»: 4Ké¢‘é“,å¤®è§†é¢‘é“,å«è§†é¢‘é“,åŒ—äº¬ä¸“å±é¢‘é“,å±±ä¸œä¸“å±é¢‘é“,æ¸¯æ¾³é¢‘é“,ç”µå½±é¢‘é“,å„¿ç«¥é¢‘é“,iHOTé¢‘é“,ç»¼åˆé¢‘é“,ä½“è‚²é¢‘é“,å‰§åœºé¢‘é“,å…¶ä»–é¢‘é“\n")
    
    print(f"âœ… æˆåŠŸç”Ÿæˆ {output_path}")
    return True

# ä»æœ¬åœ°TXTæ–‡ä»¶æå–é¢‘é“ä¿¡æ¯
def extract_channels_from_txt(file_path):
    """ä»æœ¬åœ°TXTæ–‡ä»¶æå–é¢‘é“ä¿¡æ¯"""
    channels = defaultdict(list)
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                
                # å¤„ç†åˆ†ç»„æ ‡è®°è¡Œ - åªæ”¯æŒ,#genre#æ ¼å¼
                if line.endswith(',#genre#'):
                    # æå–åˆ†ç»„åï¼šå»æ‰ ",#genre#" åç¼€
                    group_name = line[:-8].strip()  # å»æ‰ ",#genre#" (8ä¸ªå­—ç¬¦)
                    
                    # æ¸…ç†å‰åçš„#ç¬¦å·
                    while group_name.startswith('#'):
                        group_name = group_name[1:].strip()
                    while group_name.endswith('#'):
                        group_name = group_name[:-1].strip()
                    
                    # æ¸…ç†BOMå­—ç¬¦å’Œå…¶ä»–ä¸å¯è§å­—ç¬¦
                    group_name = group_name.replace('ï»¿', '').replace('\ufeff', '').strip()
                    current_group = group_name
                    continue
                
                # è·³è¿‡æ³¨é‡Šè¡Œï¼ˆä»¥#å¼€å¤´çš„è¡Œï¼‰ - ä½†è¦ç¡®ä¿åˆ†ç±»è¡Œå·²ç»å¤„ç†è¿‡äº†
                if line.startswith('#'):
                    continue
                
                # è§£æé¢‘é“ä¿¡æ¯ï¼ˆæ ¼å¼ï¼šé¢‘é“åç§°,URLï¼‰
                if ',' in line:
                    channel_name, url = line.split(',', 1)
                    channel_name = channel_name.strip()
                    url = url.strip()
                    
                    # æ£€æŸ¥é¢‘é“åæ˜¯å¦ä¸ºç©º
                    if not channel_name:
                        continue
                    
                    # æ£€æŸ¥é¢‘é“åæ˜¯å¦ä¸ºçº¯æ•°å­—
                    if channel_name.isdigit():
                        continue
                    
                    # è´­ç‰©é¢‘é“è¿‡æ»¤
                    channel_name_lower = channel_name.lower()
                    shopping_keywords = ['è´­ç‰©', 'å¯¼è´­', 'ç”µè§†è´­ç‰©']
                    if any(keyword in channel_name_lower for keyword in shopping_keywords):
                        continue
                    
                    # è·³è¿‡æ— æ•ˆçš„URLï¼ˆå…è®¸http, https, udp, rtsp, rtmpç­‰å¸¸è§æµåª’ä½“åè®®ï¼‰
                    if not url.startswith(('http://', 'https://', 'udp://', 'rtsp://', 'rtmp://', 'mms://', 'rtp://')):
                        continue
                    
                    # ç®€åŒ–å¤„ç†ï¼šç›´æ¥ä½¿ç”¨é¢‘é“åç§°ï¼Œç®€å•åˆ†ç±»
                    category = get_simple_category(channel_name)
                    channels[category].append((channel_name, url))
    except Exception as e:
        print(f"è§£ææœ¬åœ°æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}")
    
    return channels

# åŠ¨æ€è®¡ç®—æœ€ä¼˜å¹¶å‘æ•°
def get_optimal_workers():
    """åŠ¨æ€è®¡ç®—æœ€ä¼˜å¹¶å‘æ•°ï¼Œè€ƒè™‘ç³»ç»Ÿèµ„æºå’Œä»»åŠ¡ç‰¹æ€§"""
    cpu_count = multiprocessing.cpu_count()
    # æ ¹æ®ä»»åŠ¡ç±»å‹åŠ¨æ€è°ƒæ•´å¹¶å‘æ•°
    if config["url_testing"]["enable"]:
        # URLæµ‹è¯•æ˜¯I/Oå¯†é›†å‹ä»»åŠ¡ï¼Œå¯ä½¿ç”¨æ›´é«˜çš„å¹¶å‘æ•°
        # å¯¹äºæ™®é€šç³»ç»Ÿï¼ŒCPUæ ¸å¿ƒæ•° * 2 åˆ° * 4 æ˜¯æ¯”è¾ƒåˆç†çš„èŒƒå›´
        return min(64, cpu_count * 4)
    else:
        # ç›´æ’­æºè·å–æ˜¯æ··åˆä»»åŠ¡ï¼Œä½¿ç”¨é€‚ä¸­çš„å¹¶å‘æ•°
        return min(32, cpu_count * 2)

# æµ‹è¯•é¢‘é“URLæœ‰æ•ˆæ€§
def test_channels(channels):
    """æµ‹è¯•æ‰€æœ‰é¢‘é“çš„URLæœ‰æ•ˆæ€§ï¼ˆä½¿ç”¨å¿«é€Ÿæ£€æµ‹å™¨ä¼˜åŒ–ï¼‰"""
    if not config["url_testing"]["enable"]:
        print("ğŸ“Œ URLæµ‹è¯•åŠŸèƒ½å·²ç¦ç”¨")
        return channels
    
    print(f"ğŸ” å¼€å§‹æµ‹è¯•é¢‘é“URLæœ‰æ•ˆæ€§: {datetime.now(timezone(timedelta(hours=8)))}")
    
    # æ”¶é›†æ‰€æœ‰éœ€è¦æµ‹è¯•çš„é¢‘é“
    all_channel_items = []
    for category, channel_list in channels.items():
        for channel_name, url in channel_list:
            all_channel_items.append((category, channel_name, url))
    
    total_channels = len(all_channel_items)
    print(f"ğŸ“º å¾…æµ‹è¯•é¢‘é“æ€»æ•°: {total_channels}")
    
    if total_channels == 0:
        return channels
    
    # æµ‹è¯•ç»“æœ
    valid_channels = defaultdict(list)
    valid_count = 0
    invalid_count = 0
    
    # å°è¯•ä½¿ç”¨å¿«é€Ÿæ£€æµ‹å™¨
    if QUICK_CHECKER_AVAILABLE and total_channels > 50:
        print("ğŸš€ ä½¿ç”¨è½»é‡çº§å¿«é€Ÿæ£€æµ‹å™¨è¿›è¡Œæ‰¹é‡æ£€æµ‹...")
        
        try:
            # å‡†å¤‡URLåˆ—è¡¨
            urls = [(category, channel_name, url) for category, channel_name, url in all_channel_items]
            
            # åˆ›å»ºå¿«é€Ÿæ£€æµ‹å™¨
            checker = create_quick_checker(
                timeout=config["url_testing"]["timeout"],
                max_workers=min(32, config["url_testing"]["workers"]),
                enable_dns_check=True
            )
            
            # æ‰¹é‡æ£€æµ‹
            results = checker.batch_check([url for _, _, url in urls], show_progress=True)
            
            # å¤„ç†ç»“æœ
            for i, result in enumerate(results):
                category, channel_name, url = urls[i]
                
                if result['valid']:
                    valid_channels[category].append((channel_name, url))
                    valid_count += 1
                else:
                    invalid_count += 1
                    
                if (i + 1) % 100 == 0:
                    print(f"ğŸ“Š å¤„ç†è¿›åº¦: {i+1}/{len(results)} ({valid_count}æœ‰æ•ˆ, {invalid_count}æ— æ•ˆ)")
            
        except Exception as e:
            print(f"âš ï¸ å¿«é€Ÿæ£€æµ‹å™¨å‡ºé”™: {e}")
            print("ğŸ”„ å›é€€åˆ°ä¼ ç»Ÿæ£€æµ‹æ–¹å¼...")
            return test_channels_traditional(channels)
    else:
        print("ğŸ”„ ä½¿ç”¨ä¼ ç»Ÿæ£€æµ‹æ–¹å¼...")
        return test_channels_traditional(channels)
    
    print(f"âœ… URLæµ‹è¯•å®Œæˆ: {datetime.now(timezone(timedelta(hours=8)))}")
    print(f"ğŸ“Š æµ‹è¯•ç»“æœ: å…±æµ‹è¯• {total_channels} ä¸ªé¢‘é“")
    print(f"ğŸ“Š æœ‰æ•ˆé¢‘é“: {valid_count} ä¸ª")
    print(f"ğŸ“Š æ— æ•ˆé¢‘é“: {invalid_count} ä¸ª")
    print(f"ğŸ“Š æœ‰æ•ˆç‡: {valid_count/total_channels*100:.1f}%")
    
    return valid_channels

def test_channels_traditional(channels):
    """ä¼ ç»ŸURLæ£€æµ‹æ–¹æ³•ï¼ˆä½œä¸ºå›é€€æ–¹æ¡ˆï¼‰"""
    # æ”¶é›†æ‰€æœ‰éœ€è¦æµ‹è¯•çš„é¢‘é“
    all_channel_items = []
    for category, channel_list in channels.items():
        for channel_name, url in channel_list:
            all_channel_items.append((category, channel_name, url))
    
    total_channels = len(all_channel_items)
    
    # è®¡ç®—æµ‹è¯•æ‰€éœ€çš„å‚æ•°
    test_workers = config["url_testing"]["workers"]
    # é™åˆ¶æœ€å¤§çº¿ç¨‹æ•°ä¸º8ï¼Œé¿å…ç½‘ç»œå‹åŠ›è¿‡å¤§
    max_workers = min(8, test_workers if test_workers > 0 else get_optimal_workers(), len(all_channel_items))
    print(f"âš¡ ä½¿ç”¨ {max_workers} ä¸ªå¹¶å‘çº¿ç¨‹æµ‹è¯•URL...")
    
    # æµ‹è¯•ç»“æœ
    valid_channels = defaultdict(list)
    tested_count = 0
    valid_count = 0
    invalid_count = 0
    
    # æµ‹è¯•å•ä¸ªé¢‘é“URL
    def test_single_channel(channel_item):
        category, channel_name, url = channel_item
        # å¯¹äº4Ké¢‘é“ä½¿ç”¨ç¨é•¿çš„è¶…æ—¶æ—¶é—´ï¼ˆä½†ä¸è¦è¿‡é•¿ï¼‰
        timeout = 4 if is_4k(channel_name, url) else config["url_testing"]["timeout"]
        is_valid = check_url(url, timeout=timeout, retries=config["url_testing"]["retries"])
        return (category, channel_name, url, is_valid)
    
    # è®¡ç®—æ€»è¶…æ—¶æ—¶é—´ï¼ˆåŸºäºå¹¶å‘æ•°å’Œæ¯ä¸ªä»»åŠ¡çš„æœ€å¤§è¶…æ—¶æ—¶é—´ï¼‰
    total_tested = len(all_channel_items)
    base_timeout = config["url_testing"]["timeout"]
    # ä½¿ç”¨å¹¶å‘æ•°å’Œæ‰¹æ¬¡çš„æ¦‚å¿µï¼Œè€Œä¸æ˜¯ä»»åŠ¡æ€»æ•°
    # å‡è®¾æ‰€æœ‰ä»»åŠ¡åˆ†æ‰¹æ‰§è¡Œï¼Œæ¯æ‰¹æœ€å¤šmax_workersä¸ª
    batches = (total_tested + max_workers - 1) // max_workers  # å‘ä¸Šå–æ•´
    total_timeout = batches * (base_timeout + 2)  # æ¯æ‰¹æœ€å¤šè¶…æ—¶æ—¶é—´
    
    # å¹¶å‘æµ‹è¯•æ‰€æœ‰é¢‘é“
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_channel = {executor.submit(test_single_channel, item): item for item in all_channel_items}
        
        try:
            for future in as_completed(future_to_channel, timeout=total_timeout):
                category, channel_name, url = future_to_channel[future]
                try:
                    # ä¸ºå•ä¸ªfuture.result()æ·»åŠ è¶…æ—¶æ—¶é—´
                    result = future.result(timeout=base_timeout + 1)
                    category, channel_name, url, is_valid = result
                except concurrent.futures.TimeoutError:
                    print(f"âš ï¸  é¢‘é“ {channel_name} æµ‹è¯•è¶…æ—¶")
                    is_valid = False
                except Exception as e:
                    print(f"âš ï¸  æµ‹è¯•é¢‘é“ {channel_name} æ—¶å‡ºé”™: {e}")
                    is_valid = False
                
                tested_count += 1
                
                if is_valid:
                    valid_channels[category].append((channel_name, url))
                    valid_count += 1
                else:
                    invalid_count += 1
                
                # æ¯æµ‹è¯•50ä¸ªé¢‘é“æ‰“å°ä¸€æ¬¡è¿›åº¦ï¼Œæˆ–è€…å®Œæˆæ—¶æ‰“å°
                if tested_count % 50 == 0 or tested_count == total_channels:
                    print(f"ğŸ“Š æµ‹è¯•è¿›åº¦: {tested_count}/{total_channels} ({valid_count}æœ‰æ•ˆ, {invalid_count}æ— æ•ˆ) - {tested_count/total_channels*100:.1f}%")
        except concurrent.futures.TimeoutError:
            print(f"âš ï¸  URLæµ‹è¯•æ€»è¶…æ—¶ï¼Œè¿˜æœ‰ {len(future_to_channel) - tested_count} ä¸ªé¢‘é“æœªæµ‹è¯•å®Œæˆ")
        except Exception as e:
            print(f"âš ï¸  URLæµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
    
    print(f"âœ… URLæµ‹è¯•å®Œæˆ: {datetime.now(timezone(timedelta(hours=8)))}")
    print(f"ğŸ“Š æµ‹è¯•ç»“æœ: å…±æµ‹è¯• {total_channels} ä¸ªé¢‘é“")
    print(f"ğŸ“Š æœ‰æ•ˆé¢‘é“: {valid_count} ä¸ª")
    print(f"ğŸ“Š æ— æ•ˆé¢‘é“: {invalid_count} ä¸ª")
    print(f"ğŸ“Š æœ‰æ•ˆç‡: {valid_count/total_channels*100:.1f}%")
    
    return valid_channels

# å¤„ç†å•ä¸ªè¿œç¨‹ç›´æ’­æº
def process_single_source(source_url):
    """å¤„ç†å•ä¸ªè¿œç¨‹ç›´æ’­æºæˆ–æœ¬åœ°æ–‡ä»¶"""
    content = fetch_m3u_content(source_url)
    if content:
        # æ ¹æ®å†…å®¹åˆ¤æ–­æ ¼å¼
        if content.strip().startswith('#EXTM3U'):
            # M3Uæ ¼å¼
            return extract_channels_from_m3u(content)
        else:
            # TXTæ ¼å¼ï¼ˆå®‰å…¨åœ°ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶å†è§£æï¼‰
            with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False, encoding='utf-8') as f:
                temp_file_path = f.name
                f.write(content)
            
            # è®¾ç½®å®‰å…¨çš„æ–‡ä»¶æƒé™ï¼ˆä»…æ‰€æœ‰è€…å¯è¯»å†™ï¼‰
            os.chmod(temp_file_path, 0o600)
            
            try:
                return extract_channels_from_txt(temp_file_path)
            finally:
                # ç¡®ä¿æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                if os.path.exists(temp_file_path):
                    os.unlink(temp_file_path)
    return None

# åˆå¹¶ç›´æ’­æº
def merge_sources(sources, local_files):
    """åˆå¹¶å¤šä¸ªç›´æ’­æº"""
    all_channels = defaultdict(list)
    seen = set()
    
    print(f"ğŸ” å¼€å§‹åˆå¹¶ç›´æ’­æº: {datetime.now(timezone(timedelta(hours=8)))}")
    
    # å°†æœ¬åœ°æ–‡ä»¶è½¬æ¢ä¸ºfile:// URL
    local_sources = [f"file://{os.path.abspath(file_path)}" for file_path in local_files if os.path.exists(file_path)]
    
    # åˆå¹¶æ‰€æœ‰æºï¼ˆè¿œç¨‹å’Œæœ¬åœ°ï¼‰
    all_source_urls = sources + local_sources
    print(f"ï¿½ æ€»ç›´æ’­æºæ•°é‡: {len(all_source_urls)} (è¿œç¨‹: {len(sources)}, æœ¬åœ°: {len(local_sources)})")
    
    if not all_source_urls:
        print("âŒ æ²¡æœ‰å¯ç”¨çš„ç›´æ’­æº")
        return all_channels
    
    # ç»Ÿä¸€å¤„ç†æ‰€æœ‰æºï¼ˆå¹¶å‘ï¼‰
    max_workers = get_optimal_workers()
    print(f"ä½¿ç”¨ {max_workers} ä¸ªå¹¶å‘çº¿ç¨‹å¤„ç†æ‰€æœ‰ç›´æ’­æº...")
    
    remote_channel_count = 0
    local_channel_count = 0
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_source = {executor.submit(process_single_source, source_url): source_url for source_url in all_source_urls}
        
        for future in as_completed(future_to_source):
            result = future.result()
            source_url = future_to_source[future]
            
            if result:
                source_channels = sum(len(clist) for _, clist in result.items())
                
                # åˆ¤æ–­æ˜¯æœ¬åœ°æ–‡ä»¶è¿˜æ˜¯è¿œç¨‹æº
                if source_url.startswith('file://'):
                    local_channel_count += source_channels
                    print(f"âœ… æœ¬åœ°æ–‡ä»¶ {source_url[7:]} è·å–åˆ° {source_channels} ä¸ªé¢‘é“")
                else:
                    remote_channel_count += source_channels
                    print(f"âœ… è¿œç¨‹æº {source_url} è·å–åˆ° {source_channels} ä¸ªé¢‘é“")
                
                for group_title, channel_list in result.items():
                    for channel_name, url in channel_list:
                        # 4Kè¿‡æ»¤
                        if config["filter"]["only_4k"] and not is_4k(channel_name, url):
                            continue
                        # å»é‡
                        if (channel_name, url) not in seen:
                            all_channels[group_title].append((channel_name, url))
                            seen.add((channel_name, url))
            else:
                # åˆ¤æ–­æ˜¯æœ¬åœ°æ–‡ä»¶è¿˜æ˜¯è¿œç¨‹æº
                if source_url.startswith('file://'):
                    print(f"âŒ æœ¬åœ°æ–‡ä»¶ {source_url[7:]} è·å–å¤±è´¥")
                else:
                    print(f"âŒ è¿œç¨‹æº {source_url} è·å–å¤±è´¥")
    
    print(f"ğŸ“Š è¿œç¨‹ç›´æ’­æºè·å–æ€»æ•°: {remote_channel_count} ä¸ªé¢‘é“")
    print(f"ğŸ“Š æœ¬åœ°ç›´æ’­æºè·å–æ€»æ•°: {local_channel_count} ä¸ªé¢‘é“")
    print(f"ğŸ“Š åˆå¹¶åæ€»é¢‘é“æ•°: {sum(len(clist) for _, clist in all_channels.items())} ä¸ªé¢‘é“")
    
    return all_channels


# å¿½ç•¥requestsçš„SSLè­¦å‘Š
import urllib3
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

def update_iptv_sources():
    """æ›´æ–°IPTVç›´æ’­æº"""
    logger.info("ğŸš€ IPTVç›´æ’­æºè‡ªåŠ¨ç”Ÿæˆå·¥å…·")
    logger.info(f"ğŸ“… è¿è¡Œæ—¶é—´: {datetime.now(timezone(timedelta(hours=8))).strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info("=" * 50)
    
    # åŠ è½½ç¼“å­˜
    load_cache()
    
    start_time = time.time()
    
    # æ£€æŸ¥æ˜¯å¦å¯ç”¨æ¨¡æ¿é©±åŠ¨å¤„ç†
    # ä½¿ç”¨ä¼ ç»Ÿå¤„ç†æ–¹å¼ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
    logger.info("ğŸ“¡ ä½¿ç”¨ä¼ ç»Ÿå¤„ç†æ–¹å¼")
    return _update_with_traditional_method(start_time)

def _update_with_traditional_method(start_time):
    """ä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•æ›´æ–°ç›´æ’­æº"""
    # åˆå¹¶æ‰€æœ‰ç›´æ’­æº
    all_sources = config["sources"]["default"] + config["sources"]["custom"]
    logger.info(f"ğŸ“¡ æ­£åœ¨è·å–{len(all_sources)}ä¸ªè¿œç¨‹ç›´æ’­æº...")
    logger.info(f"ğŸ’» æ­£åœ¨è¯»å–{len(config['sources']['local'])}ä¸ªæœ¬åœ°ç›´æ’­æºæ–‡ä»¶...")
    
    all_channels = merge_sources(all_sources, config['sources']['local'])
    
    # æ·»åŠ è°ƒè¯•æ—¥å¿—
    logger.info(f"ğŸ” åˆå¹¶åè·å–åˆ°çš„é¢‘é“ç»„æ•°é‡: {len(all_channels)}")
    if not all_channels:
        logger.error("âŒ æ²¡æœ‰è·å–åˆ°ä»»ä½•é¢‘é“å†…å®¹ï¼")
        return False
    
    # æµ‹è¯•é¢‘é“URLæœ‰æ•ˆæ€§
    if config["url_testing"]["enable"]:
        logger.info("ğŸ” å¼€å§‹æµ‹è¯•é¢‘é“URLæœ‰æ•ˆæ€§...")
        all_channels = test_channels(all_channels)
        
        # é‡æ–°ç»Ÿè®¡é¢‘é“æ•°é‡
        total_channels = sum(len(channel_list) for channel_list in all_channels.values())
        total_groups = len(all_channels)
        
        logger.info("=" * 50)
        logger.info(f"ğŸ“Š URLæµ‹è¯•åç»Ÿè®¡:")
        logger.info(f"ğŸ“º æœ‰æ•ˆé¢‘é“ç»„æ•°: {total_groups}")
        logger.info(f"ğŸ“š æœ‰æ•ˆé¢‘é“æ€»æ•°: {total_channels}")
        logger.info(f"â±ï¸  è€—æ—¶: {format_interval(time.time() - start_time)}")
        logger.info("=" * 50)
        
        if total_channels == 0:
            logger.error("âŒ æ‰€æœ‰é¢‘é“URLæµ‹è¯•å‡æ— æ•ˆï¼")
            return False
    
    # ç»Ÿè®¡é¢‘é“æ•°é‡
    total_channels = sum(len(channel_list) for channel_list in all_channels.values())
    total_groups = len(all_channels)
    
    logger.info("=" * 50)
    logger.info(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
    logger.info(f"ğŸ“¡ ç›´æ’­æºæ•°é‡: {len(all_sources)}")
    logger.info(f"ğŸ“º é¢‘é“ç»„æ•°: {total_groups}")
    logger.info(f"ğŸ“š æ€»é¢‘é“æ•°: {total_channels}")
    logger.info(f"â±ï¸  è€—æ—¶: {format_interval(time.time() - start_time)}")
    logger.info("=" * 50)
    
    # æ˜¾ç¤ºé¢‘é“ç»„ä¿¡æ¯
    logger.info("ğŸ“‹ é¢‘é“ç»„è¯¦æƒ…:")
    for group_title, channel_list in all_channels.items():
        logger.info(f"   {group_title}: {len(channel_list)}ä¸ªé¢‘é“")
    
    # ç”ŸæˆM3Uæ–‡ä»¶ï¼ˆä½¿ç”¨å›ºå®šçš„æ—§è¾“å‡ºæ–‡ä»¶åï¼‰
    output_file_m3u = "jieguo.m3u"
    # ç”ŸæˆTXTæ–‡ä»¶ï¼ˆä½¿ç”¨å›ºå®šçš„æ—§è¾“å‡ºæ–‡ä»¶åï¼‰
    output_file_txt = "jieguo.txt"
    
    logger.info(f"ğŸ“ å‡†å¤‡ç”Ÿæˆæ–‡ä»¶: {output_file_m3u} å’Œ {output_file_txt}")
    logger.info(f"ğŸ“Š å‡†å¤‡å†™å…¥çš„é¢‘é“æ€»æ•°: {sum(len(channel_list) for channel_list in all_channels.values())}")
    
    # æ‰“å°å‰å‡ ä¸ªé¢‘é“ä½œä¸ºç¤ºä¾‹
    if all_channels:
        first_group = list(all_channels.keys())[0]
        if all_channels[first_group]:
            logger.info(f"ğŸ“º ç¤ºä¾‹é¢‘é“: {all_channels[first_group][0][0]} - {all_channels[first_group][0][1]}")
    
    success_m3u = generate_m3u_file(all_channels, output_file_m3u)
    logger.info(f"ğŸ“ M3Uæ–‡ä»¶ç”Ÿæˆç»“æœ: {'æˆåŠŸ' if success_m3u else 'å¤±è´¥'}")
    
    success_txt = generate_txt_file(all_channels, output_file_txt)
    logger.info(f"ğŸ“ TXTæ–‡ä»¶ç”Ÿæˆç»“æœ: {'æˆåŠŸ' if success_txt else 'å¤±è´¥'}")
    
    if success_m3u and success_txt:
        logger.info(f"ğŸ‰ ä»»åŠ¡å®Œæˆï¼")
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦çœŸçš„æ›´æ–°äº†
        if os.path.exists(output_file_m3u):
            mtime = os.path.getmtime(output_file_m3u)
            logger.info(f"ğŸ“… {output_file_m3u} æœ€åä¿®æ”¹æ—¶é—´: {datetime.fromtimestamp(mtime)}")
        if os.path.exists(output_file_txt):
            mtime = os.path.getmtime(output_file_txt)
            logger.info(f"ğŸ“… {output_file_txt} æœ€åä¿®æ”¹æ—¶é—´: {datetime.fromtimestamp(mtime)}")
        return True
    else:
        logger.error("ğŸ’¥ ç”Ÿæˆæ–‡ä»¶å¤±è´¥ï¼")
        return False


def check_ip_tv_syntax():
    """æ£€æŸ¥IPTV.pyæ–‡ä»¶çš„è¯­æ³•é”™è¯¯"""
    # å°è¯•è§£æå½“å‰æ–‡ä»¶ï¼Œè·å–æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
    try:
        with open(__file__, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # å°è¯•è§£ææ•´ä¸ªæ–‡ä»¶
        ast.parse(content)
        print('âœ“ IPTV.py: è¯­æ³•æ­£ç¡®')
        return True
        
    except SyntaxError as e:
        print(f'âœ— è¯­æ³•é”™è¯¯: {e}')
        print(f'è¡Œå·: {e.lineno}, åç§»é‡: {e.offset}')
        
        # è·å–æœ‰é—®é¢˜çš„è¡Œ
        lines = content.splitlines()
        if 0 <= e.lineno - 1 < len(lines):
            problem_line = lines[e.lineno - 1]
            print(f'é—®é¢˜è¡Œå†…å®¹: {repr(problem_line)}')
            
            # æ‰“å°è¯¥è¡Œçš„åå…­è¿›åˆ¶è¡¨ç¤º
            print(f'é—®é¢˜è¡Œåå…­è¿›åˆ¶: {problem_line.encode("utf-8").hex()}')
            
            # æ ‡è®°é”™è¯¯ä½ç½®
            if 0 <= e.offset - 1 < len(problem_line):
                print('é”™è¯¯ä½ç½®: ' + ' ' * (e.offset - 1) + '^')
        return False
        
    except Exception as e:
        print(f'âœ— å…¶ä»–é”™è¯¯: {type(e).__name__}: {e}')
        return False


def fix_ip_tv_chars():
    """ä¿®å¤IPTV.pyæ–‡ä»¶ä¸­çš„ä¸å¯æ‰“å°å­—ç¬¦"""
    # è¯»å–å½“å‰æ–‡ä»¶å†…å®¹
    try:
        with open(__file__, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # ç§»é™¤æ‰€æœ‰ä¸å¯æ‰“å°å­—ç¬¦ï¼ŒåŒ…æ‹¬æ¬§å…ƒç¬¦å·å’Œå…¶ä»–ç‰¹æ®Šå­—ç¬¦
        # ä¿ç•™ASCIIå¯æ‰“å°å­—ç¬¦å’Œå¸¸è§çš„ä¸­æ–‡ã€æ—¥æ–‡ã€éŸ©æ–‡ç­‰Unicodeå­—ç¬¦
        cleaned_content = CLEAN_CONTENT_PATTERN.sub('', content)
        
        # å°†æ¸…ç†åçš„å†…å®¹å†™å›æ–‡ä»¶
        with open(__file__, 'w', encoding='utf-8') as f:
            f.write(cleaned_content)
        
        print('âœ“ IPTV.pyæ–‡ä»¶ä¸­çš„ä¸å¯æ‰“å°å­—ç¬¦å·²ç§»é™¤')
        return True
        
    except Exception as e:
        print(f'âœ— å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™: {type(e).__name__}: {e}')
        return False


def validate_command_line_args():
    """éªŒè¯å‘½ä»¤è¡Œå‚æ•°çš„å®‰å…¨æ€§"""
    for arg in sys.argv[1:]:  # è·³è¿‡è„šæœ¬åç§°
        if not arg.startswith('--'):
            raise ValueError(f"å‚æ•°å¿…é¡»ä»¥'--'å¼€å¤´: {arg}")
        
        # æ£€æŸ¥å‚æ•°é•¿åº¦
        if len(arg) > 50:
            raise ValueError(f"å‚æ•°è¿‡é•¿: {arg}")
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å±é™©å­—ç¬¦
        dangerous_chars = ['<', '>', ':', '"', '|', '?', '*', ';', '&', '|', '`', '$', '(', ')', '{', '}', '[', ']', '\\']
        for char in dangerous_chars:
            if char in arg:
                raise ValueError(f"å‚æ•°åŒ…å«å±é™©å­—ç¬¦ '{char}': {arg}")

def main():
    """ä¸»å‡½æ•°"""
    import sys
    import argparse
    
    # åŠ è½½é…ç½®æ–‡ä»¶
    load_config()
    
    parser = argparse.ArgumentParser(
        description='IPTVç›´æ’­æºè‡ªåŠ¨ç”Ÿæˆå·¥å…·',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  python IPTV.py --update
  python IPTV.py --check-syntax
  python IPTV.py --fix-chars
  python IPTV.py --filter-4k
        """
    )
    
    parser.add_argument('--update', action='store_true', 
                       help='ç«‹å³æ‰‹åŠ¨æ›´æ–°ç›´æ’­æº')
    parser.add_argument('--check-syntax', action='store_true', 
                       help='æ£€æŸ¥IPTV.pyæ–‡ä»¶è¯­æ³•é”™è¯¯')
    parser.add_argument('--fix-chars', action='store_true', 
                       help='ä¿®å¤IPTV.pyæ–‡ä»¶ä¸­çš„ä¸å¯æ‰“å°å­—ç¬¦')
    parser.add_argument('--filter-4k', action='store_true', 
                       help='åªè·å–4Ké¢‘é“')
    
    try:
        # éªŒè¯å‘½ä»¤è¡Œå‚æ•°å®‰å…¨æ€§
        validate_command_line_args()
        
        # è§£æå‘½ä»¤è¡Œå‚æ•°
        args = parser.parse_args()
        
        # æ‰§è¡Œç›¸åº”æ“ä½œ
        if args.update:
            # æ‰‹åŠ¨æ›´æ–°æ¨¡å¼
            update_iptv_sources()
        elif args.check_syntax:
            # æ£€æŸ¥è¯­æ³•é”™è¯¯
            check_ip_tv_syntax()
        elif args.fix_chars:
            # ä¿®å¤ä¸å¯æ‰“å°å­—ç¬¦
            fix_ip_tv_chars()
        elif args.filter_4k:
            # åªè·å–4Ké¢‘é“æ¨¡å¼
            config["filter"]["only_4k"] = True
            update_iptv_sources()
        else:
            # æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯
            print("=" * 60)
            print("      IPTVç›´æ’­æºè‡ªåŠ¨ç”Ÿæˆå·¥å…·")
            print("=" * 60)
            print("åŠŸèƒ½ï¼š")
            print("  1. ä»å¤šä¸ªæ¥æºè·å–IPTVç›´æ’­æº")
            print("  2. ç”ŸæˆM3Uå’ŒTXTæ ¼å¼çš„ç›´æ’­æºæ–‡ä»¶")
            print("  3. æ”¯æŒæ‰‹åŠ¨æ›´æ–°å’Œé€šè¿‡GitHub Actionså·¥ä½œæµå®šæ—¶æ›´æ–°")
            print("  4. æ£€æŸ¥IPTV.pyæ–‡ä»¶è¯­æ³•é”™è¯¯")
            print("  5. ä¿®å¤IPTV.pyæ–‡ä»¶ä¸­çš„ä¸å¯æ‰“å°å­—ç¬¦")
            print("  6. æ”¯æŒåªè·å–4Ké¢‘é“")
            print("")
            print("ä½¿ç”¨æ–¹æ³•ï¼š")
            print("  python IPTV.py --update       # ç«‹å³æ‰‹åŠ¨æ›´æ–°ç›´æ’­æº")
            print("  python IPTV.py --check-syntax # æ£€æŸ¥è¯­æ³•é”™è¯¯")
            print("  python IPTV.py --fix-chars    # ä¿®å¤ä¸å¯æ‰“å°å­—ç¬¦")
            print("  python IPTV.py --filter-4k    # åªè·å–4Ké¢‘é“")
            
    except ValueError as e:
        print(f"å‚æ•°éªŒè¯é”™è¯¯: {e}")
        sys.exit(1)
    except KeyboardInterrupt:
        print("\næ“ä½œè¢«ç”¨æˆ·å–æ¶ˆ")
        sys.exit(1)
    except Exception as e:
        print(f"é”™è¯¯: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
