#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¿®å¤ç‰ˆæœ¬ç›´æ’­æºè·å–è„šæœ¬
ä¸“é—¨è§£å†³æ–°å¢ç›´æ’­æºä¸æ˜¾ç¤ºçš„é—®é¢˜
"""

import os
import re
import time
import sys
from urllib.parse import urlparse
from urllib.request import urlopen, Request
import ssl

# ç¡®ä¿UTF-8ç¼–ç 
sys.stdout = open(sys.stdout.fileno(), mode='w', encoding='utf-8', buffering=1)
sys.stderr = open(sys.stderr.fileno(), mode='w', encoding='utf-8', buffering=1)

print("å¼€å§‹æ‰§è¡Œä¿®å¤ç‰ˆç›´æ’­æºè·å–è„šæœ¬...")

# é…ç½®å‚æ•°
OUTPUT_FILE = 'CGQ.TXT'
TIMEOUT = 10  # ç§’ï¼Œé™ä½è¶…æ—¶æ—¶é—´æé«˜æ•ˆç‡

# è¯·æ±‚å¤´ï¼Œæ¨¡æ‹Ÿæµè§ˆå™¨è¡Œä¸º
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
    'Accept-Language': 'zh-CN,zh;q=0.9,en-US;q=0.8,en;q=0.7',
}

# ç¦ç”¨SSLéªŒè¯
ssl._create_default_https_context = ssl._create_unverified_context

# ç›´æ’­æºURLåˆ—è¡¨ - åŒ…å«æ–°å¢çš„ç›´æ’­æº
LIVE_SOURCES = [
    # å¯é çš„ç›´æ’­æº
    "https://iptv-org.github.io/iptv/countries/cn.m3u",
    "https://raw.githubusercontent.com/MeooPlayer/China-M3U-List/main/China_UHD.m3u",
    "https://raw.githubusercontent.com/MeooPlayer/China-M3U-List/main/China_HD.m3u",
    # å…¶ä»–ç›´æ’­æº
    "https://ghcy.eu.org/https://raw.githubusercontent.com/Supprise0901/TVBox_live/refs/heads/main/live.txt",
    "https://ghfast.top/raw.githubusercontent.com/ffmking/tv1/main/888.txt",
    "https://ghcy.eu.org/https://raw.githubusercontent.com/qingtingjjjjjjj/Web-Scraping/main/live.txt",
    "https://ghfast.top/https://raw.githubusercontent.com/kimwang1978/collect-txt/refs/heads/main/bbxx.txt",
    # æ–°å¢çš„ç›´æ’­æº
    "https://raw.githubusercontent.com/Supprise0901/tvlist/main/live.txt",
    "https://raw.githubusercontent.com/ffmking/TVlist/main/live.txt",
    "https://raw.githubusercontent.com/qingtingjjjjjjj/tvlist1/main/live.txt",
    "https://raw.githubusercontent.com/zhonghu32/live/main/888.txt",
    "https://raw.githubusercontent.com/cuijian01/dianshi/main/888.txt",
    "https://raw.githubusercontent.com/xyy0508/iptv/main/888.txt",
    "https://raw.githubusercontent.com/zhonghu32/live/main/live.txt",
    "https://raw.githubusercontent.com/cuijian01/dianshi/main/live.txt",
]

# è¶…é«˜æ¸…å…³é”®è¯
UHD_KEYWORDS = ['4K', '4k', 'è¶…é«˜æ¸…', '2160', '2160p', '8K', '8k']
HD_KEYWORDS = ['HD', '1080p', 'é«˜æ¸…']

# é¢‘é“åˆ†ç±»
CHANNEL_CATEGORIES = {
    "å¤®è§†": ['CCTV', 'ä¸­å¤®ç”µè§†å°'],
    "å«è§†": ['å«è§†', 'æ¹–å—å«è§†', 'æµ™æ±Ÿå«è§†', 'æ±Ÿè‹å«è§†', 'ä¸œæ–¹å«è§†', 'åŒ—äº¬å«è§†', 'å¹¿ä¸œå«è§†'],
    "ç”µå½±": ['ç”µå½±', 'CHC', 'Movie', 'Film'],
    "ä½“è‚²": ['ä½“è‚²', 'è¶³çƒ', 'ç¯®çƒ', 'NBA', 'CCTV5', 'sports'],
    "å„¿ç«¥": ['å°‘å„¿', 'å¡é€š', 'åŠ¨ç”»', 'Cartoon', 'Kids'],
    "4Kå¤®è§†é¢‘é“": ['CCTV', '4K'],
    "4Kè¶…é«˜æ¸…é¢‘é“": ['4Kè¶…é«˜æ¸…', '4Kä¸“åŒº'],
    "é«˜æ¸…é¢‘é“": ['HD', '1080p'],
}

def is_valid_url(url):
    """éªŒè¯URLæ˜¯å¦æœ‰æ•ˆï¼Œæ›´å®½æ¾çš„éªŒè¯ç­–ç•¥"""
    try:
        result = urlparse(url)
        return bool(result.scheme) and bool(result.netloc)
    except:
        return False

def clean_url(url):
    """æ¸…ç†URLä¸­çš„å¼‚å¸¸æ ¼å¼"""
    # å¤„ç†é‡å¤åè®®å‰ç¼€
    if 'https://https://' in url:
        url = url.replace('https://https://', 'https://')
    elif 'http://https://' in url:
        url = url.replace('http://https://', 'https://')
    elif 'https://http://' in url:
        url = url.replace('https://http://', 'http://')
    elif 'http://http://' in url:
        url = url.replace('http://http://', 'http://')
    
    # å»é™¤é¦–å°¾ç©ºç™½å­—ç¬¦
    url = url.strip()
    return url

def get_source_content(url):
    """è·å–ç›´æ’­æºå†…å®¹ï¼Œå¢åŠ é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶"""
    print(f"æ­£åœ¨è·å–ç›´æ’­æº: {url}")
    
    # æ¸…ç†URL
    url = clean_url(url)
    
    for retry in range(2):  # æœ€å¤šé‡è¯•1æ¬¡
        try:
            req = Request(url, headers=HEADERS)
            with urlopen(req, timeout=TIMEOUT) as response:
                content = response.read()
                print(f"  æˆåŠŸè·å–ï¼Œå¤§å°: {len(content)} å­—èŠ‚")
                
                # å°è¯•è§£ç 
                try:
                    return content.decode('utf-8')
                except UnicodeDecodeError:
                    try:
                        return content.decode('latin-1')
                    except:
                        print(f"  è§£ç å¤±è´¥ï¼Œè·³è¿‡æ­¤æº")
                        return None
        except Exception as e:
            print(f"  è·å–å¤±è´¥ (å°è¯• {retry+1}/2): {str(e)}")
            if retry == 0:
                print("  æ­£åœ¨é‡è¯•...")
                time.sleep(1)
    
    return None

def is_uhd_content(name, url):
    """åˆ¤æ–­æ˜¯å¦ä¸ºè¶…é«˜æ¸…å†…å®¹"""
    combined = (name + ' ' + url).lower()
    for keyword in UHD_KEYWORDS:
        if keyword.lower() in combined:
            return True
    return False

def extract_channels_from_m3u(content):
    """ä»M3Uæ ¼å¼å†…å®¹æå–é¢‘é“"""
    channels = []
    if not content:
        return channels
    
    lines = content.split('\n')
    extinf_line = None
    
    for line in lines:
        line = line.strip()
        if line.startswith('#EXTINF:'):
            extinf_line = line
        elif line.startswith(('http://', 'https://', 'udp://', 'rtmp://', 'rtsp://')) and extinf_line:
            # æå–é¢‘é“åç§°
            try:
                channel_name = extinf_line.split(',')[-1].strip()
                url = line
                is_uhd = is_uhd_content(channel_name, url)
                channels.append((channel_name, url, is_uhd))
            except:
                pass
            extinf_line = None
    
    return channels

def extract_channels_from_txt(content):
    """ä»ç®€å•æ–‡æœ¬æ ¼å¼æå–é¢‘é“"""
    channels = []
    if not content:
        return channels
    
    lines = content.split('\n')
    
    # å°è¯•å¤šç§æ ¼å¼ï¼šname\nurl æˆ– name,url
    i = 0
    while i < len(lines):
        line = lines[i].strip()
        if not line or line.startswith('#'):
            i += 1
            continue
        
        # å°è¯•name,urlæ ¼å¼
        if ',' in line and line.count(',') == 1:
            try:
                name, url = line.split(',', 1)
                name = name.strip()
                url = url.strip()
                if name and is_valid_url(url):
                    is_uhd = is_uhd_content(name, url)
                    channels.append((name, url, is_uhd))
            except:
                pass
        # å°è¯•name\nurlæ ¼å¼
        elif i + 1 < len(lines):
            next_line = lines[i + 1].strip()
            if next_line.startswith(('http://', 'https://', 'udp://', 'rtmp://', 'rtsp://')):
                name = line
                url = next_line
                if name and is_valid_url(url):
                    is_uhd = is_uhd_content(name, url)
                    channels.append((name, url, is_uhd))
                i += 1
        
        i += 1
    
    return channels

def categorize_channel(channel_name):
    """å¯¹é¢‘é“è¿›è¡Œåˆ†ç±»"""
    for category, keywords in CHANNEL_CATEGORIES.items():
        for keyword in keywords:
            if keyword in channel_name:
                return category
    return "å…¶ä»–é¢‘é“"

def process_all_live_sources():
    """å¤„ç†æ‰€æœ‰ç›´æ’­æº"""
    all_channels = []
    
    print(f"\nå¼€å§‹å¤„ç† {len(LIVE_SOURCES)} ä¸ªç›´æ’­æº...")
    
    for i, url in enumerate(LIVE_SOURCES, 1):
        print(f"\n[{i}/{len(LIVE_SOURCES)}] å¤„ç†: {url}")
        
        if not is_valid_url(url):
            print(f"  URLæ ¼å¼æ— æ•ˆï¼Œè·³è¿‡")
            continue
        
        content = get_source_content(url)
        if not content:
            print(f"  æœªè·å–åˆ°å†…å®¹ï¼Œè·³è¿‡")
            continue
        
        # å°è¯•ä¸åŒæ ¼å¼æå–é¢‘é“
        channels = []
        if '#EXTM3U' in content:
            print(f"  æ£€æµ‹åˆ°M3Uæ ¼å¼")
            channels = extract_channels_from_m3u(content)
        else:
            print(f"  æ£€æµ‹åˆ°æ–‡æœ¬æ ¼å¼")
            channels = extract_channels_from_txt(content)
        
        if channels:
            print(f"  æˆåŠŸæå– {len(channels)} ä¸ªé¢‘é“")
            all_channels.extend(channels)
        else:
            print(f"  æœªæå–åˆ°ä»»ä½•æœ‰æ•ˆé¢‘é“")
    
    print(f"\næ€»å…±è·å–åˆ° {len(all_channels)} ä¸ªåŸå§‹é¢‘é“")
    
    # å»é‡ - ä½¿ç”¨æ›´å®½æ¾çš„ç­–ç•¥ï¼Œä¿ç•™ä¸åŒURLçš„åŒä¸€é¢‘é“
    print("\nå¼€å§‹å»é‡å¤„ç†...")
    unique_channels = {}
    for name, url, is_uhd in all_channels:
        # ä½¿ç”¨åç§°å’ŒURLçš„ç»„åˆä½œä¸ºå»é‡é”®
        key = f"{name}|{url}"
        if key not in unique_channels:
            unique_channels[key] = (name, url, is_uhd)
    
    all_channels = list(unique_channels.values())
    print(f"å»é‡åå‰©ä½™ {len(all_channels)} ä¸ªå”¯ä¸€é¢‘é“")
    
    # æŒ‰åˆ†ç±»ç»„ç»‡é¢‘é“
    categorized = {}
    for name, url, is_uhd in all_channels:
        cat = categorize_channel(name)
        if cat not in categorized:
            categorized[cat] = []
        categorized[cat].append((name, url, is_uhd))
    
    print(f"\né¢‘é“åˆ†ç±»ç»“æœ:")
    for cat, chans in categorized.items():
        print(f"  {cat}: {len(chans)} ä¸ªé¢‘é“")
    
    return categorized

def write_to_file(categorized_channels):
    """å°†é¢‘é“å†™å…¥CGQ.TXTæ–‡ä»¶"""
    try:
        lines = []
        
        # æ–‡ä»¶å¤´
        lines.append(f"# è¶…é«˜æ¸…ç›´æ’­æºåˆ—è¡¨")
        lines.append(f"# æ›´æ–°æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}")
        lines.append(f"# å…±åŒ…å« {sum(len(chans) for chans in categorized_channels.values())} ä¸ªé¢‘é“")
        lines.append("")
        
        # åˆ†ç±»æ’åºä¼˜å…ˆçº§
        category_order = ["4Kå¤®è§†é¢‘é“", "4Kè¶…é«˜æ¸…é¢‘é“", "é«˜æ¸…é¢‘é“", "å¤®è§†", "å«è§†", "ä½“è‚²", "ç”µå½±", "å„¿ç«¥", "å…¶ä»–é¢‘é“"]
        
        # å†™å…¥æ¯ä¸ªåˆ†ç±»
        for category in category_order:
            if category in categorized_channels:
                # æ·»åŠ åˆ†ç±»æ ‡è®°
                lines.append(f"{category},#genre#")
                
                # æ’åºé¢‘é“ï¼šUHDä¼˜å…ˆï¼Œç„¶åæŒ‰åç§°æ’åº
                channels = sorted(categorized_channels[category], key=lambda x: (not x[2], x[0]))
                
                for name, url, is_uhd in channels:
                    if is_valid_url(url):
                        lines.append(f"{name},{url}")
                
                # åˆ†ç±»ä¹‹é—´ç©ºè¡Œ
                lines.append("")
        
        # å†™å…¥æ–‡ä»¶
        with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
            f.write('\n'.join(lines))
        
        print(f"\nâœ“ æˆåŠŸå†™å…¥ {OUTPUT_FILE}")
        print(f"  å…± {len(lines)} è¡Œæ•°æ®")
        return True
    except Exception as e:
        print(f"\nâœ— å†™å…¥æ–‡ä»¶å¤±è´¥: {str(e)}")
        return False

def main():
    """ä¸»å‡½æ•°"""
    print(f"Pythonç‰ˆæœ¬: {sys.version}")
    print(f"å½“å‰ç›®å½•: {os.getcwd()}")
    
    start_time = time.time()
    
    try:
        # å¤„ç†æ‰€æœ‰ç›´æ’­æº
        categorized_channels = process_all_live_sources()
        
        # å¦‚æœæ²¡æœ‰è·å–åˆ°é¢‘é“ï¼Œæä¾›ä¸€äº›é»˜è®¤é¢‘é“
        if not categorized_channels:
            print("\nè­¦å‘Š: æœªèƒ½ä»ç½‘ç»œè·å–åˆ°ç›´æ’­æºæ•°æ®")
            categorized_channels = {
                "4Kå¤®è§†é¢‘é“": [("CCTV-4Kè¶…é«˜æ¸…", "https://tv.cctv.com/live/cctv4k/", True)],
                "é«˜æ¸…é¢‘é“": [
                    ("CCTV-1ç»¼åˆ", "https://tv.cctv.com/live/cctv1/", False),
                    ("CCTV-2è´¢ç»", "https://tv.cctv.com/live/cctv2/", False),
                ]
            }
        
        # å†™å…¥æ–‡ä»¶
        if write_to_file(categorized_channels):
            elapsed = time.time() - start_time
            print(f"\nğŸ‰ ç›´æ’­æºæ›´æ–°å®Œæˆï¼")
            print(f"æ€»è€—æ—¶: {elapsed:.2f} ç§’")
            return 0
        else:
            return 1
    
    except KeyboardInterrupt:
        print("\nç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­")
        return 130
    except Exception as e:
        print(f"\nç¨‹åºé”™è¯¯: {type(e).__name__}")
        print(f"é”™è¯¯ä¿¡æ¯: {str(e)}")
        import traceback
        print("è¯¦ç»†é”™è¯¯å †æ ˆ:")
        traceback.print_exc()
        return 1

if __name__ == "__main__":
    sys.exit(main())
